{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transfer from dBW to W (power)\n",
    "def db2pow(db):\n",
    "    return 10**(db/10)\n",
    "\n",
    "# Function to transfer from W to dBW (power)\n",
    "def pow2db(pow):\n",
    "    return 10*np.log10(pow)\n",
    "\n",
    "# Hermitian transpose of a matrix\n",
    "def HermTranspose(x):\n",
    "    return x.conj().T\n",
    "\n",
    "def chanGen(zeta, d, dim1, dim2):\n",
    "    \"\"\"Function to generate Rayleigh fading channel coefficients\n",
    "\n",
    "    Args:\n",
    "        zeta: Î¾ is the path loss exponent\n",
    "        d: the distance between the transmitter and the receiver\n",
    "        dim1: the number of rows in the channel matrix\n",
    "        dim2: the number of columns in the channel matrix\n",
    "    \"\"\"\n",
    "    pl_ref = -30                                    # pathloss (dBW) at reference distance\n",
    "    pl = db2pow(pl_ref - 10*zeta*np.log10(d))       # pathloss model at distance d\n",
    "    y = np.sqrt(0.5*pl)*(np.random.randn(dim1,dim2)\\\n",
    "        + 1j*np.random.randn(dim1,dim2))            # Rayleigh distribution\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_beamforming_vectors():\n",
    "    # Generate random complex numbers for each element of the beamforming vector\n",
    "    beamforming_vectors = [(np.random.randn(N, 1) + 1j * np.random.randn(N, 1)) for i in range (number_of_users)]\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    for i in range (number_of_users):\n",
    "        beamforming_vectors[i] = beamforming_vectors[i] / np.linalg.norm(beamforming_vectors[i])\n",
    "    return beamforming_vectors\n",
    "    #w: list of beamforming vectors, length = number of users, elements are N x 1\n",
    "\n",
    "def generate_random_theta():\n",
    "    realPart = np.random.randn(1, Nris)\n",
    "    imagPart = np.random.randn(1, Nris)\n",
    "    theta = realPart + 1j * imagPart\n",
    "    theta = np.exp(1j * np.angle(theta))  # Normalize theta to the unit circle\n",
    "    return theta\n",
    "    #theta: phase shift of RIS, size 1 x Nris\n",
    "\n",
    "def generateChannel():\n",
    "    normFact = 1/np.sqrt(sigma)\n",
    "    Hai = chanGen(zetaAI, dAI, Nris, N)                                                         # Alice to RIS channel\n",
    "    hib = [normFact*chanGen(zetaIB, dIB[i], 1, Nris) for i in range(number_of_users)]           # Channel between the RIS and the legitimate receivers\n",
    "    hie = [normFact*chanGen(zetaIE, dIE[i], 1, Nris) for i in range(number_of_eavesdroppers)]   # Channel between the RIS and the eavesdroppers\n",
    "    hab = [normFact*chanGen(zetaAB, dAB[i], 1, N) for i in range(number_of_users)]              # Channel between Alice and the legitimate receivers\n",
    "    hae = [normFact*chanGen(zetaAE, dAE[i], 1, N) for i in range(number_of_eavesdroppers)]      # Channel between Alice and the eavesdroppers\n",
    "    return Hai, hib, hie, hab, hae\n",
    "\n",
    "    #Hai: Channel between Alice and RIS: Nris x N  \n",
    "    #hib: Channel between RIS and users: List of length number_of_users, elements are 1 x Nris\n",
    "    #hab: Channel between Alice and users: List of length number_of_users, elements are 1 x N\n",
    "    #hie: Channel between RIS and eavesdroppers: List of length number_of_eavesdroppers, elements are 1 x Nris\n",
    "    #hae: Channel between Alice and eavesdroppers: List of length number_of_eavesdroppers, elements are 1 x N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secrecy_rate_objective_function(theta, w):\n",
    "    secrecy_rate = 0\n",
    "    for k in range(number_of_users):\n",
    "        R_bk = []\n",
    "        # Legitimate user k\n",
    "        Z_bk = hib[k] @ np.diag(theta.flatten()) @ Hai + hab[k]\n",
    "        numGamma_bk = np.abs(Z_bk @ w[k])**2\n",
    "        denGamma_bk = 1 + np.sum([np.abs(Z_bk @ w[i])**2 for i in range(number_of_users) if i != k])\n",
    "        gamma_bk = numGamma_bk/denGamma_bk\n",
    "        C_bk = np.log2(1 + gamma_bk)\n",
    "        \n",
    "        for m in range(number_of_eavesdroppers):\n",
    "            # Eavesdropper i\n",
    "            Z_em = hie[m] @ np.diag(theta.flatten()) @ Hai + hae[m]\n",
    "            numGamma_em = np.abs(Z_em @ w[k])**2\n",
    "            denGamma_em = 1 + np.sum([np.abs(Z_em @ w[j])**2 for j in range(number_of_users) if j != k])\n",
    "            gamma_em = numGamma_em/denGamma_em\n",
    "            C_em = np.log2(1 + gamma_em)\n",
    "            R_bk.append(C_bk - C_em)\n",
    "        \n",
    "        secrecy_rate += max(min(R_bk),0)\n",
    "    return secrecy_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = db2pow(-75)                                                                 # noise power\n",
    "N = 8                                                                               # number of transmit antennas\n",
    "Nris = 32                                                                           # number of RIS elements\n",
    "number_of_users = 4                                                                 # number of users\n",
    "number_of_eavesdroppers = 2                                                         # number of eavesdroppers\n",
    "zetaAI = 2.2                                                                        # Path loss exponent of the channel between the Alice and the RIS\n",
    "zetaIB = 2.5                                                                        # Path loss exponent of the channel between the legitimate receivers and the RIS\n",
    "zetaIE = 2.5                                                                        # Path loss exponent of the channel between the eavesdroppers and the RIS\n",
    "zetaAB = 3.5                                                                        # Path loss exponent of the channel between the Alice and the legitimate receivers\n",
    "zetaAE = 3.5                                                                        # Path loss exponent of the channel between the Alice and the eavesdroppers\n",
    "\n",
    "dAI = 50                                                                            # distance between Alice and the RIS\n",
    "dv = 2                                                                              # Vertical distance between the Alice and the Eve and Bob\n",
    "dABh = np.random.uniform(5, 10, size=number_of_users)                               # Horizontal distance between Alice and the legitimate receivers\n",
    "dAEh = np.random.uniform(50, 150, size=number_of_eavesdroppers)                     # Horizontal distance between Alice and the eavesdroppers\n",
    "dAB = [np.sqrt(dABh[i]**2 + dv**2) for i in range(number_of_users)]                 # Distance between Alice and the legitimate receivers\n",
    "dAE = [np.sqrt(dAEh[i]**2 + dv**2) for i in range(number_of_eavesdroppers)]         # Distance between Alice and the eavesdroppers\n",
    "dIB = [np.sqrt((dABh[i]-dAI)**2 + dv**2) for i in range(number_of_users)]           # Distance between the legitimate receivers and the RIS\n",
    "dIE = [np.sqrt((dAEh[i]-dAI)**2 + dv**2) for i in range(number_of_eavesdroppers)]   # Distance between the eavesdroppers and the RIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.95856413-0.28487684j  0.98944252-0.14492587j -0.23973511-0.97083834j\n",
      "   0.56851402-0.82267357j  0.93003612-0.36746812j -0.09884125-0.99510321j\n",
      "  -0.28609081-0.95820251j  0.79591454-0.60540898j  0.33606125-0.94184013j\n",
      "   0.60676734+0.79487948j -0.71162637+0.70255811j  0.81234481+0.58317743j\n",
      "   0.93865085+0.34486895j  0.98913673-0.1469984j   0.59880851-0.80089223j\n",
      "  -0.99756748-0.06970746j  0.78225257-0.6229614j  -0.86567663-0.50060361j\n",
      "  -0.62344516-0.78186708j  0.56718117+0.82359305j -0.11826569+0.99298199j\n",
      "   0.69630365-0.71774733j  0.49301394+0.87002141j -0.91963413+0.3927761j\n",
      "  -0.11491485+0.99337535j  0.17407055+0.98473318j -0.58677238-0.80975192j\n",
      "  -0.1839127 +0.98294258j -0.01428081+0.99989802j  0.30230897-0.95320999j\n",
      "  -0.69859727+0.71551509j  0.62138921-0.78350204j]]\n",
      "distance between Alice and the receivers:  [9.079615659903945, 5.481638765280457, 8.409528160779496, 8.969831408089735]\n",
      "distance between Alice and the eavesdroppers:  [99.87072912607805, 72.50725324909014]\n",
      "Secrecy Rate: [[1.19110391]]\n"
     ]
    }
   ],
   "source": [
    "# Channel generation\n",
    "Hai, hib, hie, hab, hae = generateChannel()\n",
    "\n",
    "# Generate random theta and w\n",
    "theta_init = generate_random_theta()\n",
    "w_init = generate_random_beamforming_vectors()\n",
    "\n",
    "print(theta_init)\n",
    "print(\"distance between Alice and the receivers: \", dAB)\n",
    "print(\"distance between Alice and the eavesdroppers: \", dAE)\n",
    "print(\"Secrecy Rate:\", secrecy_rate_objective_function(theta_init, w_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Maximization (GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_w(theta, w):\n",
    "    grad_w = []\n",
    "    Z_e_max = [] #Z_e_max[k] = Z_e_max for user k\n",
    "    gamma_e_max = [] #gamma_e_max[k] = gamma_e_max for user k\n",
    "    Z_b = [] #Z_b[k] = Z_bk\n",
    "    gamma_b = [] #gamma_b[k] = gamma_bk\n",
    "    counted = [] #counted[k] = true if gamma_k > gamma_e_max[k]\n",
    "    \n",
    "\n",
    "    #Precalculation \n",
    "    for k in range (number_of_users):\n",
    "        gamma_e = []\n",
    "        for m in range (number_of_eavesdroppers):\n",
    "            Z_em = hie[m] @ np.diag(theta.flatten()) @ Hai + hae[m]\n",
    "            numGamma_em = np.abs(Z_em @ w[k])**2\n",
    "            denGamma_em = 1 + np.sum([np.abs(Z_em @ w[j])**2 for j in range(number_of_users) if j != k])\n",
    "            gamma_em = numGamma_em/denGamma_em\n",
    "            gamma_e.append(gamma_em)\n",
    "        \n",
    "        gamma_e_max.append(max(gamma_e))\n",
    "        index_e_max = gamma_e.index(max(gamma_e))\n",
    "        Z_e = hie[index_e_max] @ np.diag(theta.flatten()) @ Hai + hae[index_e_max]\n",
    "        Z_e_max.append(Z_e)\n",
    "\n",
    "        Z_bk = hib[k] @ np.diag(theta.flatten()) @ Hai + hab[k]\n",
    "        numGamma_bk = np.abs(Z_bk @ w[k])**2\n",
    "        denGamma_bk = 1 + np.sum([np.abs(Z_bk @ w[i])**2 for i in range(number_of_users) if i != k])\n",
    "        gamma_bk = numGamma_bk/denGamma_bk\n",
    "\n",
    "        Z_b.append(Z_bk)\n",
    "        gamma_b.append(gamma_bk)\n",
    "\n",
    "        counted.append(gamma_bk > gamma_e[index_e_max])\n",
    "\n",
    "\n",
    "    #Calculating grad for i-th beamforming vector\n",
    "    for i in range (number_of_users):\n",
    "        grad = np.zeros((N, 1))\n",
    "        #print(\"grad_shape =\", grad.shape)\n",
    "        for k in range (number_of_users):\n",
    "            if (counted[k] == False):\n",
    "                continue\n",
    "            if (k == i):\n",
    "                num_grad_C_bk_to_w_k = 2 * (HermTranspose(Z_b[k]) @ Z_b[k] @ w[k])\n",
    "                den_grad_C_bk_to_w_k = (1 + gamma_b[k]) * np.log(2) * (1 + sum([abs(Z_b[k] @ w[j]) for j in range (number_of_users) if j != k]))\n",
    "                grad_C_bk_to_w_k = num_grad_C_bk_to_w_k / den_grad_C_bk_to_w_k\n",
    "\n",
    "                num_grad_C_e_max_to_w_k = 2 * (HermTranspose(Z_e_max[k]) @ Z_e_max[k] @ w[k])\n",
    "                den_grad_C_e_max_to_w_k = (1 + gamma_e_max[k]) * np.log(2) * (1 + sum([abs(Z_e_max[k] @ w[j]) for j in range (number_of_users) if j != k]))\n",
    "                grad_C_e_max_to_w_k = num_grad_C_e_max_to_w_k / den_grad_C_e_max_to_w_k\n",
    "                \n",
    "                #print(\"num_grad_C_e_max_to_w_k\", num_grad_C_e_max_to_w_k.shape)\n",
    "                #print(\"den_grad_C_e_max_to_w_k\", den_grad_C_e_max_to_w_k.shape)\n",
    "                grad = grad - (grad_C_bk_to_w_k - grad_C_e_max_to_w_k)\n",
    "            else:\n",
    "                num_grad_C_bk_to_w_i = -2 * abs(Z_b[k] @ w[k]) * (HermTranspose(Z_b[k]) @ Z_b[k] @ w[i])\n",
    "                den_grad_C_bk_to_w_i = (1 + gamma_b[k]) * np.log(2) * (1 + sum([abs(Z_b[k] @ w[j]) for j in range (number_of_users) if j != k])) ** 2\n",
    "                grad_C_bk_to_w_i = num_grad_C_bk_to_w_i / den_grad_C_bk_to_w_i\n",
    "\n",
    "                num_grad_C_e_max_to_w_i = -2 * abs(Z_e_max[k] @ w[k]) * (HermTranspose(Z_e_max[k]) @ Z_e_max[k] @ w[i])\n",
    "                den_grad_C_e_max_to_w_i = (1 + gamma_e_max[k]) * np.log(2) * (1 + sum([abs(Z_e_max[k] @ w[j]) for j in range (number_of_users) if j != k])) ** 2\n",
    "                grad_C_e_max_to_w_i = num_grad_C_e_max_to_w_i / den_grad_C_e_max_to_w_i\n",
    "\n",
    "                grad = grad - (grad_C_bk_to_w_i - grad_C_e_max_to_w_i)\n",
    "            \n",
    "        grad_w.append(grad)\n",
    "    return grad_w\n",
    "\n",
    "def compute_gradient_theta(theta, w, epsilon=1e-3):\n",
    "    grad_theta = []\n",
    "    for i in range(Nris):\n",
    "        theta_plus = theta.copy()\n",
    "        theta_plus[0, i] += epsilon + epsilon*1j\n",
    "        theta_minus = theta.copy()\n",
    "        theta_minus[0, i] -= epsilon + epsilon*1j\n",
    "        grad_theta_i = (secrecy_rate_objective_function(theta_plus, w) - secrecy_rate_objective_function(theta_minus, w))/(2*epsilon)\n",
    "        grad_theta.append(grad_theta_i)\n",
    "            \n",
    "    return np.array(grad_theta).reshape(1, Nris)\n",
    "\n",
    "\n",
    "\n",
    "def gradient_descent_update(w, theta, learning_rate):\n",
    "    grad_w = compute_gradient_w(theta, w)\n",
    "    grad_theta = compute_gradient_theta(theta, w)\n",
    "    w_new = [w[i] - learning_rate * grad_w[i] for i in range (number_of_users)]\n",
    "    for i in range (number_of_users):\n",
    "        w_new[i] = w_new[i] / np.linalg.norm(w_new[i])\n",
    "        \n",
    "    theta_new = theta - learning_rate * grad_theta\n",
    "    theta_new = np.exp(1j * np.angle(theta_new))\n",
    "\n",
    "    return w_new, theta_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Secrecy Rate GD: [[1.19110391]]\n",
      "[[1.19110391]]\n",
      "[[4.24294725]]\n",
      "[[4.24294725]]\n",
      "[[4.82480366]]\n",
      "[[4.82480366]]\n",
      "[[4.94382893]]\n",
      "[[4.94382893]]\n",
      "[[4.45370871]]\n",
      "Converged\n",
      "[array([[1.19110391]]), array([[4.24294725]]), array([[4.82480366]]), array([[4.94382893]])]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent Algorithm\n",
    "num_cycles = 500\n",
    "learning_rate = 0.5\n",
    "theta_GD = theta_init.copy()\n",
    "w_GD = w_init.copy()\n",
    "print(\"Initial Secrecy Rate GD:\", secrecy_rate_objective_function(theta_GD, w_GD))\n",
    "current_secrecy_rate = secrecy_rate_objective_function(theta_init, w_init)\n",
    "\n",
    "GD_results = []\n",
    "GD_results.append(current_secrecy_rate)\n",
    "\n",
    "for i in range(num_cycles):\n",
    "    #print(w_GD[0].shape)\n",
    "    w_new, theta_new = gradient_descent_update(w_GD, theta_GD, learning_rate)\n",
    "    #print(\"____________________________-\")\n",
    "    #print(w_new[0].shape)\n",
    "    new_secrecy_rate = secrecy_rate_objective_function(theta_new, w_new)\n",
    "    print(current_secrecy_rate)\n",
    "    print(new_secrecy_rate)\n",
    "    if (new_secrecy_rate - current_secrecy_rate) < 1e-6:\n",
    "        print(\"Converged\")\n",
    "        break\n",
    "    w_GD = w_new\n",
    "    theta_GD = theta_new\n",
    "    GD_results.append(new_secrecy_rate)\n",
    "    current_secrecy_rate = new_secrecy_rate\n",
    "print(GD_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Swarm Optimization (PSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self):\n",
    "        self.theta = theta_PSO.copy()\n",
    "        self.w = w_PSO.copy()\n",
    "        self.pbest_theta = self.theta.copy()\n",
    "        self.pbest_w = self.w.copy()\n",
    "        self.pbest_value = secrecy_rate_objective_function(self.theta, self.w)\n",
    "        self.velocity_theta = generate_random_theta()\n",
    "        self.velocity_w = generate_random_beamforming_vectors()\n",
    "\n",
    "    def update_velocity_theta(self, gbest_theta, inertia =0.5, c1=1.5, c2=2.0, user_k=0):\n",
    "        r1, r2 = np.random.rand(), np.random.rand()\n",
    "        cognitive_velocity_theta = c1 * r1 * (self.pbest_theta[user_k] - self.theta[user_k])\n",
    "        social_velocity_theta = c2 * r2 * (gbest_theta[user_k] - self.theta[user_k])\n",
    "        self.velocity_theta[user_k] = inertia * self.velocity_theta[user_k] + cognitive_velocity_theta + social_velocity_theta\n",
    "\n",
    "    def update_velocity_w(self, gbest_w, inertia = 0.5, c1=1.5, c2=2.0, user_k=0):\n",
    "        r1, r2 = np.random.rand(), np.random.rand()\n",
    "        cognitive_velocity_w = c1 * r1 * (self.pbest_w[user_k] - self.w[user_k])\n",
    "        social_velocity_w = c2 * r2 * (gbest_w[user_k] - self.w[user_k])\n",
    "        self.velocity_w[user_k] = inertia * self.velocity_w[user_k] + cognitive_velocity_w + social_velocity_w\n",
    "    \n",
    "    def update_position_theta(self, user_k=0):\n",
    "        self.theta[user_k] += self.velocity_theta[user_k]\n",
    "        self.theta[user_k] = np.exp(1j * np.angle(self.theta[user_k]))\n",
    "    \n",
    "    def update_position_w(self):\n",
    "        self.w += self.velocity_w\n",
    "        self.w = self.w / np.linalg.norm(self.w, axis=1, keepdims=True)\n",
    "\n",
    "def PSO_optimize_theta(w, max_iter=100):\n",
    "    particles = [Particle() for _ in range(number_of_users)]\n",
    "    gbest_theta = particles[0].theta.copy()\n",
    "    gbest_value = particles[0].pbest_value.copy()\n",
    "    \n",
    "    w_max = 0.9\n",
    "    w_min = 0.4\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        E_t = float((max_iter - iteration - 1)/max_iter)\n",
    "        inertia = w_min + (w_max - w_min) * (2 /(1 + (np.e ** (-5 * E_t))) - 1)\n",
    "\n",
    "        for k in range(number_of_users):\n",
    "            particles[k].update_velocity_theta(gbest_theta, inertia, user_k=k)\n",
    "            particles[k].update_position_theta(user_k=k)\n",
    "                        \n",
    "            fitness_value = secrecy_rate_objective_function(particles[k].theta, w)\n",
    "            \n",
    "            if fitness_value > particles[k].pbest_value:\n",
    "                particles[k].pbest_value = fitness_value\n",
    "                particles[k].pbest_theta = particles[k].theta.copy()\n",
    "            \n",
    "            if fitness_value > gbest_value:\n",
    "                gbest_value = fitness_value\n",
    "                gbest_theta = particles[k].theta.copy()\n",
    "        # print(f\"Iteration {iteration+1}/{max_iter}, Global Best Value: {gbest_value}\\n\")\n",
    "    return gbest_theta\n",
    "\n",
    "def PSO_optimize_w(theta, max_iter=100):\n",
    "    particles = [Particle() for _ in range(number_of_users)]\n",
    "    gbest_w = particles[0].w.copy()\n",
    "    gbest_value = particles[0].pbest_value.copy()\n",
    "    \n",
    "    w_max = 0.9\n",
    "    w_min = 0.4\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        inertia = w_min + (w_max - w_min) * np.random.rand()\n",
    "\n",
    "        for k in range(number_of_users):\n",
    "            particles[k].update_velocity_w(gbest_w, inertia, user_k=k)\n",
    "            particles[k].update_position_w()\n",
    "            \n",
    "            fitness_value = secrecy_rate_objective_function(theta, particles[k].w)\n",
    "            \n",
    "            if fitness_value > particles[k].pbest_value:\n",
    "                particles[k].pbest_value = fitness_value\n",
    "                particles[k].pbest_w = particles[k].w.copy()\n",
    "            \n",
    "            if fitness_value > gbest_value:\n",
    "                gbest_value = fitness_value\n",
    "                gbest_w = particles[k].w.copy()\n",
    "        # print(f\"Iteration {iteration+1}/{max_iter}, Global Best Value: {gbest_value}\\n\")\n",
    "    return gbest_w\n",
    "\n",
    "def PSO_optimize_w_theta(max_iter=100):\n",
    "    particles = [Particle() for _ in range(number_of_users)]\n",
    "    gbest_theta = particles[0].theta.copy()\n",
    "    gbest_w = particles[0].w.copy()\n",
    "    gbest_value = particles[0].pbest_value.copy()\n",
    "    \n",
    "    w_max = 0.9\n",
    "    w_min = 0.4\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        E_t = float((max_iter - iteration - 1)/max_iter)\n",
    "        inertia = w_min + (w_max - w_min) * (2 /(1 + (np.e ** (-5 * E_t))) - 1)\n",
    "\n",
    "        for k in range(number_of_users):\n",
    "            particles[k].update_velocity_theta(gbest_theta, inertia, user_k=k)\n",
    "            particles[k].update_position_theta(user_k=k)\n",
    "            particles[k].update_velocity_w(gbest_w, inertia, user_k=k)\n",
    "            particles[k].update_position_w()\n",
    "            \n",
    "            fitness_value = secrecy_rate_objective_function(particles[k].theta, particles[k].w)\n",
    "            \n",
    "            if fitness_value > particles[k].pbest_value:\n",
    "                particles[k].pbest_value = fitness_value\n",
    "                particles[k].pbest_theta = particles[k].theta.copy()\n",
    "                particles[k].pbest_w = particles[k].w.copy()\n",
    "            \n",
    "            if fitness_value > gbest_value:\n",
    "                gbest_value = fitness_value\n",
    "                gbest_theta = particles[k].theta.copy()\n",
    "                gbest_w = particles[k].w.copy()\n",
    "        # print(f\"Iteration {iteration+1}/{max_iter}, Global Best Value: {gbest_value}\\n\")\n",
    "    return gbest_theta, gbest_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Secrecy Rate PSO: [[1.19110391]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[284], line 22\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# for cycle in range(num_cycles):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     theta_opt = PSO_optimize_theta(w_PSO, max_iter=total_iter)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#     theta_PSO = theta_opt\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     if (cycle + 1) % 10 == 0:\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#         PSO_results.append(secrecy_rate_objective_function(theta_opt, w_opt))\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cycle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_cycles):\n\u001b[0;32m---> 22\u001b[0m     theta_PSO, w_PSO \u001b[38;5;241m=\u001b[39m \u001b[43mPSO_optimize_w_theta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCycle \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcycle\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cycles\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Secret Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msecrecy_rate_objective_function(theta_PSO,\u001b[38;5;250m \u001b[39mw_PSO)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (cycle \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[283], line 100\u001b[0m, in \u001b[0;36mPSO_optimize_w_theta\u001b[0;34m(max_iter)\u001b[0m\n\u001b[1;32m     97\u001b[0m inertia \u001b[38;5;241m=\u001b[39m w_min \u001b[38;5;241m+\u001b[39m (w_max \u001b[38;5;241m-\u001b[39m w_min) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (np\u001b[38;5;241m.\u001b[39me \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m*\u001b[39m E_t))) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_users):\n\u001b[0;32m--> 100\u001b[0m     \u001b[43mparticles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_velocity_theta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgbest_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minertia\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     particles[k]\u001b[38;5;241m.\u001b[39mupdate_position_theta(user_k\u001b[38;5;241m=\u001b[39mk)\n\u001b[1;32m    102\u001b[0m     particles[k]\u001b[38;5;241m.\u001b[39mupdate_velocity_w(gbest_w, inertia, user_k\u001b[38;5;241m=\u001b[39mk)\n",
      "Cell \u001b[0;32mIn[283], line 13\u001b[0m, in \u001b[0;36mParticle.update_velocity_theta\u001b[0;34m(self, gbest_theta, inertia, c1, c2, user_k)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_velocity_theta\u001b[39m(\u001b[38;5;28mself\u001b[39m, gbest_theta, inertia \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, c1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, c2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, user_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     12\u001b[0m     r1, r2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(), np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand()\n\u001b[0;32m---> 13\u001b[0m     cognitive_velocity_theta \u001b[38;5;241m=\u001b[39m c1 \u001b[38;5;241m*\u001b[39m r1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpbest_theta\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_k\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta[user_k])\n\u001b[1;32m     14\u001b[0m     social_velocity_theta \u001b[38;5;241m=\u001b[39m c2 \u001b[38;5;241m*\u001b[39m r2 \u001b[38;5;241m*\u001b[39m (gbest_theta[user_k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta[user_k])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvelocity_theta[user_k] \u001b[38;5;241m=\u001b[39m inertia \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvelocity_theta[user_k] \u001b[38;5;241m+\u001b[39m cognitive_velocity_theta \u001b[38;5;241m+\u001b[39m social_velocity_theta\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "total_iter = 100\n",
    "num_cycles = 500\n",
    "\n",
    "theta_PSO = theta_init.copy()\n",
    "w_PSO = w_init.copy()\n",
    "\n",
    "print(\"Initial Secrecy Rate PSO:\", secrecy_rate_objective_function(theta_PSO, w_PSO))\n",
    "PSO_results = []\n",
    "PSO_results.append(secrecy_rate_objective_function(theta_PSO, w_PSO))\n",
    "\n",
    "# for cycle in range(num_cycles):\n",
    "#     theta_opt = PSO_optimize_theta(w_PSO, max_iter=total_iter)\n",
    "#     theta_PSO = theta_opt\n",
    "#     w_opt = PSO_optimize_w(theta_opt, max_iter=total_iter)\n",
    "#     w_PSO = w_opt\n",
    "#     print(f\"Cycle {cycle+1}/{num_cycles}, Secret Rate: {secrecy_rate_objective_function(theta_opt, w_opt)}\")\n",
    "    \n",
    "#     if (cycle + 1) % 10 == 0:\n",
    "#         PSO_results.append(secrecy_rate_objective_function(theta_opt, w_opt))\n",
    "\n",
    "for cycle in range(num_cycles):\n",
    "    theta_PSO, w_PSO = PSO_optimize_w_theta(max_iter=total_iter)\n",
    "    print(f\"Cycle {cycle+1}/{num_cycles}, Secret Rate: {secrecy_rate_objective_function(theta_PSO, w_PSO)}\")\n",
    "    \n",
    "    if (cycle + 1) % 10 == 0:\n",
    "        PSO_results.append(secrecy_rate_objective_function(theta_PSO, w_PSO))\n",
    "\n",
    "print(PSO_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm (GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Secrecy Rate GA: [2.02873527]\n"
     ]
    }
   ],
   "source": [
    "class Individual:\n",
    "    def __init__(self):\n",
    "        self.theta = theta_init.copy()\n",
    "        self.w = w_init.copy() \n",
    "        \n",
    "def evaluate_population(population):\n",
    "    fitness = []\n",
    "    for individual in population:\n",
    "        theta, w = individual.theta, individual.w\n",
    "        fitness.append(secrecy_rate_objective_function(theta, w))\n",
    "    # print(\"Fitness:\", fitness)\n",
    "    return fitness\n",
    "\n",
    "def select_parents(population, fitness):\n",
    "    combined = list(zip(fitness, population))\n",
    "    sorted_combined = sorted(combined, key=lambda x: x[0], reverse=True)\n",
    "    sorted_population = [x[1] for x in sorted_combined]\n",
    "    return sorted_population[:2]\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    theta1, w1 = parent1.theta, parent1.w\n",
    "    theta2, w2 = parent2.theta, parent2.w\n",
    "    child_theta = (theta1 + theta2) / 2\n",
    "    child_w = (w1 + w2) / 2\n",
    "    new_individual = Individual()\n",
    "    new_individual.theta = child_theta\n",
    "    new_individual.w = child_w\n",
    "    return new_individual\n",
    "\n",
    "def mutate(individual):\n",
    "    if np.random.rand() < mutation_rate:\n",
    "        mutation_index = np.random.randint(len(individual.theta))\n",
    "        individual.theta[mutation_index] = generate_random_theta()[mutation_index]\n",
    "    if np.random.rand() < mutation_rate:\n",
    "        mutation_index = np.random.randint(len(individual.w))\n",
    "        individual.w[mutation_index] = generate_random_beamforming_vectors()[mutation_index]\n",
    "    return individual\n",
    "        \n",
    "def genetic_algorithm():\n",
    "    population = [Individual() for _ in range(population_size)]\n",
    "    best_individual = None\n",
    "    best_fitness = -np.inf\n",
    "    \n",
    "    for generation in range(num_generations):\n",
    "        fitness = evaluate_population(population)\n",
    "        current_best_fitness = max(fitness)\n",
    "        if current_best_fitness > best_fitness:\n",
    "            best_fitness = current_best_fitness\n",
    "            best_individual = population[np.argmax(fitness)]\n",
    "        \n",
    "        parents = select_parents(population, fitness)\n",
    "        new_population = []\n",
    "        for _ in range(population_size):\n",
    "            parent1, parent2 = parents\n",
    "            child = crossover(parent1, parent2)\n",
    "            child = mutate(child)\n",
    "            new_population.append(child)\n",
    "        \n",
    "        population = new_population\n",
    "        print(f\"Generation {generation + 1}/{num_generations}, Best Fitness: {best_fitness}\")\n",
    "        \n",
    "        if (generation + 1) % 10 == 0:\n",
    "            GA_results.append(best_fitness)\n",
    "\n",
    "    return best_individual\n",
    "\n",
    "print(\"Initial Secrecy Rate GA:\", secrecy_rate_objective_function(theta_init, w_init))\n",
    "\n",
    "# Genetic Algorithm parameters\n",
    "population_size = 100\n",
    "num_generations = 500\n",
    "mutation_rate = 0.2\n",
    "\n",
    "GA_results = []\n",
    "GA_results.append(secrecy_rate_objective_function(theta_init, w_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/500, Best Fitness: [2.02873527]\n",
      "Generation 2/500, Best Fitness: [3.22005965]\n",
      "Generation 3/500, Best Fitness: [4.24173074]\n",
      "Generation 4/500, Best Fitness: [4.63506144]\n",
      "Generation 5/500, Best Fitness: [5.208986]\n",
      "Generation 6/500, Best Fitness: [8.10646328]\n",
      "Generation 7/500, Best Fitness: [8.10646328]\n",
      "Generation 8/500, Best Fitness: [8.10646328]\n",
      "Generation 9/500, Best Fitness: [8.19563442]\n",
      "Generation 10/500, Best Fitness: [8.19563442]\n",
      "Generation 11/500, Best Fitness: [8.19563442]\n",
      "Generation 12/500, Best Fitness: [8.19563442]\n",
      "Generation 13/500, Best Fitness: [8.19563442]\n",
      "Generation 14/500, Best Fitness: [8.19563442]\n",
      "Generation 15/500, Best Fitness: [8.21943292]\n",
      "Generation 16/500, Best Fitness: [8.4888554]\n",
      "Generation 17/500, Best Fitness: [9.06377355]\n",
      "Generation 18/500, Best Fitness: [9.06377355]\n",
      "Generation 19/500, Best Fitness: [9.06377355]\n",
      "Generation 20/500, Best Fitness: [9.06377355]\n",
      "Generation 21/500, Best Fitness: [9.06377355]\n",
      "Generation 22/500, Best Fitness: [9.06377355]\n",
      "Generation 23/500, Best Fitness: [9.06377355]\n",
      "Generation 24/500, Best Fitness: [9.06377355]\n",
      "Generation 25/500, Best Fitness: [9.06377355]\n",
      "Generation 26/500, Best Fitness: [9.06377355]\n",
      "Generation 27/500, Best Fitness: [9.06377355]\n",
      "Generation 28/500, Best Fitness: [9.06377355]\n",
      "Generation 29/500, Best Fitness: [9.27857257]\n",
      "Generation 30/500, Best Fitness: [9.27857257]\n",
      "Generation 31/500, Best Fitness: [9.67295821]\n",
      "Generation 32/500, Best Fitness: [9.67745964]\n",
      "Generation 33/500, Best Fitness: [9.67866302]\n",
      "Generation 34/500, Best Fitness: [9.68097371]\n",
      "Generation 35/500, Best Fitness: [9.96511564]\n",
      "Generation 36/500, Best Fitness: [9.96511564]\n",
      "Generation 37/500, Best Fitness: [9.96511564]\n",
      "Generation 38/500, Best Fitness: [9.96511564]\n",
      "Generation 39/500, Best Fitness: [9.96511564]\n",
      "Generation 40/500, Best Fitness: [9.96511564]\n",
      "Generation 41/500, Best Fitness: [9.96511564]\n",
      "Generation 42/500, Best Fitness: [9.96511564]\n",
      "Generation 43/500, Best Fitness: [9.96511564]\n",
      "Generation 44/500, Best Fitness: [9.96511564]\n",
      "Generation 45/500, Best Fitness: [9.96511564]\n",
      "Generation 46/500, Best Fitness: [9.96511564]\n",
      "Generation 47/500, Best Fitness: [9.96511564]\n",
      "Generation 48/500, Best Fitness: [9.96511564]\n",
      "Generation 49/500, Best Fitness: [9.96511564]\n",
      "Generation 50/500, Best Fitness: [9.96511564]\n",
      "Generation 51/500, Best Fitness: [9.96511564]\n",
      "Generation 52/500, Best Fitness: [9.96511564]\n",
      "Generation 53/500, Best Fitness: [9.96511564]\n",
      "Generation 54/500, Best Fitness: [9.96511564]\n",
      "Generation 55/500, Best Fitness: [9.96511564]\n",
      "Generation 56/500, Best Fitness: [9.96511564]\n",
      "Generation 57/500, Best Fitness: [9.96511564]\n",
      "Generation 58/500, Best Fitness: [9.96511564]\n",
      "Generation 59/500, Best Fitness: [9.96511564]\n",
      "Generation 60/500, Best Fitness: [9.96511564]\n",
      "Generation 61/500, Best Fitness: [10.20663154]\n",
      "Generation 62/500, Best Fitness: [10.20663154]\n",
      "Generation 63/500, Best Fitness: [10.20663154]\n",
      "Generation 64/500, Best Fitness: [10.20663154]\n",
      "Generation 65/500, Best Fitness: [10.20663154]\n",
      "Generation 66/500, Best Fitness: [10.20663154]\n",
      "Generation 67/500, Best Fitness: [10.20663154]\n",
      "Generation 68/500, Best Fitness: [10.20663154]\n",
      "Generation 69/500, Best Fitness: [10.20663154]\n",
      "Generation 70/500, Best Fitness: [10.20663154]\n",
      "Generation 71/500, Best Fitness: [10.20663154]\n",
      "Generation 72/500, Best Fitness: [10.20663154]\n",
      "Generation 73/500, Best Fitness: [10.20663154]\n",
      "Generation 74/500, Best Fitness: [10.20663154]\n",
      "Generation 75/500, Best Fitness: [10.20663154]\n",
      "Generation 76/500, Best Fitness: [10.20663154]\n",
      "Generation 77/500, Best Fitness: [10.20663154]\n",
      "Generation 78/500, Best Fitness: [10.20663154]\n",
      "Generation 79/500, Best Fitness: [10.20663154]\n",
      "Generation 80/500, Best Fitness: [10.20663154]\n",
      "Generation 81/500, Best Fitness: [10.20663154]\n",
      "Generation 82/500, Best Fitness: [10.20663154]\n",
      "Generation 83/500, Best Fitness: [10.20663154]\n",
      "Generation 84/500, Best Fitness: [10.20663154]\n",
      "Generation 85/500, Best Fitness: [10.20663154]\n",
      "Generation 86/500, Best Fitness: [10.20663154]\n",
      "Generation 87/500, Best Fitness: [10.20663154]\n",
      "Generation 88/500, Best Fitness: [10.20663154]\n",
      "Generation 89/500, Best Fitness: [10.20663154]\n",
      "Generation 90/500, Best Fitness: [10.20663154]\n",
      "Generation 91/500, Best Fitness: [10.20663154]\n",
      "Generation 92/500, Best Fitness: [10.20663154]\n",
      "Generation 93/500, Best Fitness: [10.20663154]\n",
      "Generation 94/500, Best Fitness: [10.20663154]\n",
      "Generation 95/500, Best Fitness: [10.20663154]\n",
      "Generation 96/500, Best Fitness: [10.20663154]\n",
      "Generation 97/500, Best Fitness: [10.20663154]\n",
      "Generation 98/500, Best Fitness: [10.20663154]\n",
      "Generation 99/500, Best Fitness: [10.20663154]\n",
      "Generation 100/500, Best Fitness: [10.20663154]\n",
      "Generation 101/500, Best Fitness: [10.20663154]\n",
      "Generation 102/500, Best Fitness: [10.20663154]\n",
      "Generation 103/500, Best Fitness: [10.20663154]\n",
      "Generation 104/500, Best Fitness: [10.20663154]\n",
      "Generation 105/500, Best Fitness: [10.20663154]\n",
      "Generation 106/500, Best Fitness: [10.20663154]\n",
      "Generation 107/500, Best Fitness: [10.20663154]\n",
      "Generation 108/500, Best Fitness: [10.20663154]\n",
      "Generation 109/500, Best Fitness: [10.20663154]\n",
      "Generation 110/500, Best Fitness: [10.20663154]\n",
      "Generation 111/500, Best Fitness: [10.20663154]\n",
      "Generation 112/500, Best Fitness: [10.20663154]\n",
      "Generation 113/500, Best Fitness: [10.20663154]\n",
      "Generation 114/500, Best Fitness: [10.20663154]\n",
      "Generation 115/500, Best Fitness: [10.20663154]\n",
      "Generation 116/500, Best Fitness: [10.40229816]\n",
      "Generation 117/500, Best Fitness: [10.40229816]\n",
      "Generation 118/500, Best Fitness: [10.40229816]\n",
      "Generation 119/500, Best Fitness: [10.40229816]\n",
      "Generation 120/500, Best Fitness: [10.40229816]\n",
      "Generation 121/500, Best Fitness: [10.40229816]\n",
      "Generation 122/500, Best Fitness: [10.40229816]\n",
      "Generation 123/500, Best Fitness: [10.73375295]\n",
      "Generation 124/500, Best Fitness: [10.73375295]\n",
      "Generation 125/500, Best Fitness: [10.73375295]\n",
      "Generation 126/500, Best Fitness: [10.73375295]\n",
      "Generation 127/500, Best Fitness: [10.73375295]\n",
      "Generation 128/500, Best Fitness: [10.73375295]\n",
      "Generation 129/500, Best Fitness: [10.73375295]\n",
      "Generation 130/500, Best Fitness: [10.73375295]\n",
      "Generation 131/500, Best Fitness: [10.73375295]\n",
      "Generation 132/500, Best Fitness: [10.73375295]\n",
      "Generation 133/500, Best Fitness: [10.73375295]\n",
      "Generation 134/500, Best Fitness: [10.73375295]\n",
      "Generation 135/500, Best Fitness: [10.73375295]\n",
      "Generation 136/500, Best Fitness: [10.73375295]\n",
      "Generation 137/500, Best Fitness: [10.73375295]\n",
      "Generation 138/500, Best Fitness: [10.73375295]\n",
      "Generation 139/500, Best Fitness: [10.73375295]\n",
      "Generation 140/500, Best Fitness: [10.73375295]\n",
      "Generation 141/500, Best Fitness: [10.73375295]\n",
      "Generation 142/500, Best Fitness: [10.73375295]\n",
      "Generation 143/500, Best Fitness: [10.73375295]\n",
      "Generation 144/500, Best Fitness: [10.73375295]\n",
      "Generation 145/500, Best Fitness: [10.73375295]\n",
      "Generation 146/500, Best Fitness: [10.73375295]\n",
      "Generation 147/500, Best Fitness: [10.73375295]\n",
      "Generation 148/500, Best Fitness: [10.73375295]\n",
      "Generation 149/500, Best Fitness: [10.73375295]\n",
      "Generation 150/500, Best Fitness: [10.73375295]\n",
      "Generation 151/500, Best Fitness: [10.73375295]\n",
      "Generation 152/500, Best Fitness: [10.73375295]\n",
      "Generation 153/500, Best Fitness: [10.73375295]\n",
      "Generation 154/500, Best Fitness: [10.73375295]\n",
      "Generation 155/500, Best Fitness: [10.73375295]\n",
      "Generation 156/500, Best Fitness: [10.73375295]\n",
      "Generation 157/500, Best Fitness: [10.73375295]\n",
      "Generation 158/500, Best Fitness: [10.73375295]\n",
      "Generation 159/500, Best Fitness: [10.73375295]\n",
      "Generation 160/500, Best Fitness: [10.73375295]\n",
      "Generation 161/500, Best Fitness: [10.73375295]\n",
      "Generation 162/500, Best Fitness: [10.73375295]\n",
      "Generation 163/500, Best Fitness: [10.73375295]\n",
      "Generation 164/500, Best Fitness: [10.73375295]\n",
      "Generation 165/500, Best Fitness: [10.73375295]\n",
      "Generation 166/500, Best Fitness: [10.73375295]\n",
      "Generation 167/500, Best Fitness: [10.73375295]\n",
      "Generation 168/500, Best Fitness: [10.73375295]\n",
      "Generation 169/500, Best Fitness: [10.73375295]\n",
      "Generation 170/500, Best Fitness: [10.73375295]\n",
      "Generation 171/500, Best Fitness: [10.73375295]\n",
      "Generation 172/500, Best Fitness: [10.73375295]\n",
      "Generation 173/500, Best Fitness: [10.73375295]\n",
      "Generation 174/500, Best Fitness: [10.73375295]\n",
      "Generation 175/500, Best Fitness: [10.73375295]\n",
      "Generation 176/500, Best Fitness: [10.73375295]\n",
      "Generation 177/500, Best Fitness: [10.73375295]\n",
      "Generation 178/500, Best Fitness: [10.73375295]\n",
      "Generation 179/500, Best Fitness: [10.73375295]\n",
      "Generation 180/500, Best Fitness: [10.73375295]\n",
      "Generation 181/500, Best Fitness: [10.73375295]\n",
      "Generation 182/500, Best Fitness: [10.73375295]\n",
      "Generation 183/500, Best Fitness: [10.73375295]\n",
      "Generation 184/500, Best Fitness: [10.73375295]\n",
      "Generation 185/500, Best Fitness: [10.73375295]\n",
      "Generation 186/500, Best Fitness: [10.73375295]\n",
      "Generation 187/500, Best Fitness: [10.73375295]\n",
      "Generation 188/500, Best Fitness: [10.73375295]\n",
      "Generation 189/500, Best Fitness: [10.73375295]\n",
      "Generation 190/500, Best Fitness: [10.73375295]\n",
      "Generation 191/500, Best Fitness: [10.73375295]\n",
      "Generation 192/500, Best Fitness: [10.73375295]\n",
      "Generation 193/500, Best Fitness: [10.73375295]\n",
      "Generation 194/500, Best Fitness: [10.73375295]\n",
      "Generation 195/500, Best Fitness: [10.73375295]\n",
      "Generation 196/500, Best Fitness: [10.73375295]\n",
      "Generation 197/500, Best Fitness: [10.73375295]\n",
      "Generation 198/500, Best Fitness: [10.73375295]\n",
      "Generation 199/500, Best Fitness: [10.73375295]\n",
      "Generation 200/500, Best Fitness: [10.73375295]\n",
      "Generation 201/500, Best Fitness: [10.73375295]\n",
      "Generation 202/500, Best Fitness: [10.73375295]\n",
      "Generation 203/500, Best Fitness: [10.73375295]\n",
      "Generation 204/500, Best Fitness: [10.73375295]\n",
      "Generation 205/500, Best Fitness: [10.73375295]\n",
      "Generation 206/500, Best Fitness: [10.73375295]\n",
      "Generation 207/500, Best Fitness: [10.73375295]\n",
      "Generation 208/500, Best Fitness: [10.73375295]\n",
      "Generation 209/500, Best Fitness: [10.73375295]\n",
      "Generation 210/500, Best Fitness: [10.73375295]\n",
      "Generation 211/500, Best Fitness: [10.73375295]\n",
      "Generation 212/500, Best Fitness: [10.73375295]\n",
      "Generation 213/500, Best Fitness: [10.73375295]\n",
      "Generation 214/500, Best Fitness: [10.73375295]\n",
      "Generation 215/500, Best Fitness: [10.73375295]\n",
      "Generation 216/500, Best Fitness: [10.73375295]\n",
      "Generation 217/500, Best Fitness: [10.73375295]\n",
      "Generation 218/500, Best Fitness: [10.73375295]\n",
      "Generation 219/500, Best Fitness: [10.73375295]\n",
      "Generation 220/500, Best Fitness: [10.73375295]\n",
      "Generation 221/500, Best Fitness: [10.73375295]\n",
      "Generation 222/500, Best Fitness: [10.73375295]\n",
      "Generation 223/500, Best Fitness: [10.73375295]\n",
      "Generation 224/500, Best Fitness: [10.73375295]\n",
      "Generation 225/500, Best Fitness: [10.73375295]\n",
      "Generation 226/500, Best Fitness: [10.73375295]\n",
      "Generation 227/500, Best Fitness: [10.73375295]\n",
      "Generation 228/500, Best Fitness: [10.73375295]\n",
      "Generation 229/500, Best Fitness: [10.73375295]\n",
      "Generation 230/500, Best Fitness: [10.73375295]\n",
      "Generation 231/500, Best Fitness: [10.73375295]\n",
      "Generation 232/500, Best Fitness: [10.73375295]\n",
      "Generation 233/500, Best Fitness: [10.73375295]\n",
      "Generation 234/500, Best Fitness: [10.73375295]\n",
      "Generation 235/500, Best Fitness: [10.73375295]\n",
      "Generation 236/500, Best Fitness: [10.73375295]\n",
      "Generation 237/500, Best Fitness: [10.73375295]\n",
      "Generation 238/500, Best Fitness: [10.73375295]\n",
      "Generation 239/500, Best Fitness: [10.73375295]\n",
      "Generation 240/500, Best Fitness: [10.73375295]\n",
      "Generation 241/500, Best Fitness: [10.73375295]\n",
      "Generation 242/500, Best Fitness: [10.73375295]\n",
      "Generation 243/500, Best Fitness: [10.73375295]\n",
      "Generation 244/500, Best Fitness: [10.73375295]\n",
      "Generation 245/500, Best Fitness: [10.73375295]\n",
      "Generation 246/500, Best Fitness: [10.73375295]\n",
      "Generation 247/500, Best Fitness: [10.73375295]\n",
      "Generation 248/500, Best Fitness: [10.73375295]\n",
      "Generation 249/500, Best Fitness: [10.73375295]\n",
      "Generation 250/500, Best Fitness: [10.73375295]\n",
      "Generation 251/500, Best Fitness: [10.73375295]\n",
      "Generation 252/500, Best Fitness: [10.73375295]\n",
      "Generation 253/500, Best Fitness: [10.73375295]\n",
      "Generation 254/500, Best Fitness: [10.73375295]\n",
      "Generation 255/500, Best Fitness: [10.73375295]\n",
      "Generation 256/500, Best Fitness: [10.73375295]\n",
      "Generation 257/500, Best Fitness: [10.73375295]\n",
      "Generation 258/500, Best Fitness: [10.73375295]\n",
      "Generation 259/500, Best Fitness: [10.73375295]\n",
      "Generation 260/500, Best Fitness: [10.73375295]\n",
      "Generation 261/500, Best Fitness: [10.73375295]\n",
      "Generation 262/500, Best Fitness: [10.73375295]\n",
      "Generation 263/500, Best Fitness: [10.73375295]\n",
      "Generation 264/500, Best Fitness: [10.73375295]\n",
      "Generation 265/500, Best Fitness: [10.73375295]\n",
      "Generation 266/500, Best Fitness: [10.73375295]\n",
      "Generation 267/500, Best Fitness: [10.73375295]\n",
      "Generation 268/500, Best Fitness: [10.73375295]\n",
      "Generation 269/500, Best Fitness: [10.73375295]\n",
      "Generation 270/500, Best Fitness: [10.73375295]\n",
      "Generation 271/500, Best Fitness: [10.73375295]\n",
      "Generation 272/500, Best Fitness: [10.73375295]\n",
      "Generation 273/500, Best Fitness: [10.73375295]\n",
      "Generation 274/500, Best Fitness: [10.73375295]\n",
      "Generation 275/500, Best Fitness: [10.73375295]\n",
      "Generation 276/500, Best Fitness: [10.73375295]\n",
      "Generation 277/500, Best Fitness: [10.73375295]\n",
      "Generation 278/500, Best Fitness: [10.73375295]\n",
      "Generation 279/500, Best Fitness: [10.73375295]\n",
      "Generation 280/500, Best Fitness: [10.73375295]\n",
      "Generation 281/500, Best Fitness: [10.73375295]\n",
      "Generation 282/500, Best Fitness: [10.73375295]\n",
      "Generation 283/500, Best Fitness: [10.73375295]\n",
      "Generation 284/500, Best Fitness: [10.73375295]\n",
      "Generation 285/500, Best Fitness: [10.73375295]\n",
      "Generation 286/500, Best Fitness: [10.73375295]\n",
      "Generation 287/500, Best Fitness: [10.73375295]\n",
      "Generation 288/500, Best Fitness: [10.73375295]\n",
      "Generation 289/500, Best Fitness: [10.73375295]\n",
      "Generation 290/500, Best Fitness: [10.73375295]\n",
      "Generation 291/500, Best Fitness: [10.73375295]\n",
      "Generation 292/500, Best Fitness: [10.73375295]\n",
      "Generation 293/500, Best Fitness: [10.73375295]\n",
      "Generation 294/500, Best Fitness: [10.73375295]\n",
      "Generation 295/500, Best Fitness: [10.73375295]\n",
      "Generation 296/500, Best Fitness: [10.73375295]\n",
      "Generation 297/500, Best Fitness: [10.73375295]\n",
      "Generation 298/500, Best Fitness: [10.73375295]\n",
      "Generation 299/500, Best Fitness: [10.73375295]\n",
      "Generation 300/500, Best Fitness: [10.73375295]\n",
      "Generation 301/500, Best Fitness: [10.73375295]\n",
      "Generation 302/500, Best Fitness: [10.73375295]\n",
      "Generation 303/500, Best Fitness: [10.73375295]\n",
      "Generation 304/500, Best Fitness: [10.73375295]\n",
      "Generation 305/500, Best Fitness: [10.73375295]\n",
      "Generation 306/500, Best Fitness: [10.73375295]\n",
      "Generation 307/500, Best Fitness: [10.73375295]\n",
      "Generation 308/500, Best Fitness: [10.73375295]\n",
      "Generation 309/500, Best Fitness: [10.73375295]\n",
      "Generation 310/500, Best Fitness: [10.73375295]\n",
      "Generation 311/500, Best Fitness: [10.73375295]\n",
      "Generation 312/500, Best Fitness: [10.73375295]\n",
      "Generation 313/500, Best Fitness: [10.73375295]\n",
      "Generation 314/500, Best Fitness: [10.73375295]\n",
      "Generation 315/500, Best Fitness: [10.73375295]\n",
      "Generation 316/500, Best Fitness: [10.73375295]\n",
      "Generation 317/500, Best Fitness: [10.73375295]\n",
      "Generation 318/500, Best Fitness: [10.73375295]\n",
      "Generation 319/500, Best Fitness: [10.73375295]\n",
      "Generation 320/500, Best Fitness: [10.73375295]\n",
      "Generation 321/500, Best Fitness: [10.73375295]\n",
      "Generation 322/500, Best Fitness: [10.73375295]\n",
      "Generation 323/500, Best Fitness: [10.73375295]\n",
      "Generation 324/500, Best Fitness: [10.73375295]\n",
      "Generation 325/500, Best Fitness: [10.73375295]\n",
      "Generation 326/500, Best Fitness: [10.73375295]\n",
      "Generation 327/500, Best Fitness: [10.73375295]\n",
      "Generation 328/500, Best Fitness: [10.73375295]\n",
      "Generation 329/500, Best Fitness: [10.73375295]\n",
      "Generation 330/500, Best Fitness: [10.73375295]\n",
      "Generation 331/500, Best Fitness: [10.73375295]\n",
      "Generation 332/500, Best Fitness: [10.73375295]\n",
      "Generation 333/500, Best Fitness: [10.73375295]\n",
      "Generation 334/500, Best Fitness: [10.73375295]\n",
      "Generation 335/500, Best Fitness: [10.73375295]\n",
      "Generation 336/500, Best Fitness: [10.73375295]\n",
      "Generation 337/500, Best Fitness: [10.73375295]\n",
      "Generation 338/500, Best Fitness: [10.73375295]\n",
      "Generation 339/500, Best Fitness: [10.73375295]\n",
      "Generation 340/500, Best Fitness: [10.73375295]\n",
      "Generation 341/500, Best Fitness: [10.73375295]\n",
      "Generation 342/500, Best Fitness: [10.73375295]\n",
      "Generation 343/500, Best Fitness: [10.73375295]\n",
      "Generation 344/500, Best Fitness: [10.73375295]\n",
      "Generation 345/500, Best Fitness: [10.73375295]\n",
      "Generation 346/500, Best Fitness: [10.73375295]\n",
      "Generation 347/500, Best Fitness: [10.73375295]\n",
      "Generation 348/500, Best Fitness: [11.03943571]\n",
      "Generation 349/500, Best Fitness: [11.41850335]\n",
      "Generation 350/500, Best Fitness: [11.41850335]\n",
      "Generation 351/500, Best Fitness: [11.41850335]\n",
      "Generation 352/500, Best Fitness: [11.41850335]\n",
      "Generation 353/500, Best Fitness: [11.41850335]\n",
      "Generation 354/500, Best Fitness: [11.6046905]\n",
      "Generation 355/500, Best Fitness: [11.6046905]\n",
      "Generation 356/500, Best Fitness: [11.6046905]\n",
      "Generation 357/500, Best Fitness: [11.6046905]\n",
      "Generation 358/500, Best Fitness: [11.6046905]\n",
      "Generation 359/500, Best Fitness: [11.6046905]\n",
      "Generation 360/500, Best Fitness: [11.6046905]\n",
      "Generation 361/500, Best Fitness: [11.6046905]\n",
      "Generation 362/500, Best Fitness: [11.6046905]\n",
      "Generation 363/500, Best Fitness: [11.6046905]\n",
      "Generation 364/500, Best Fitness: [11.6046905]\n",
      "Generation 365/500, Best Fitness: [11.6046905]\n",
      "Generation 366/500, Best Fitness: [11.6046905]\n",
      "Generation 367/500, Best Fitness: [11.6046905]\n",
      "Generation 368/500, Best Fitness: [11.6046905]\n",
      "Generation 369/500, Best Fitness: [11.6046905]\n",
      "Generation 370/500, Best Fitness: [11.6046905]\n",
      "Generation 371/500, Best Fitness: [11.6046905]\n",
      "Generation 372/500, Best Fitness: [11.6046905]\n",
      "Generation 373/500, Best Fitness: [11.6046905]\n",
      "Generation 374/500, Best Fitness: [11.6046905]\n",
      "Generation 375/500, Best Fitness: [11.6046905]\n",
      "Generation 376/500, Best Fitness: [11.6046905]\n",
      "Generation 377/500, Best Fitness: [11.6046905]\n",
      "Generation 378/500, Best Fitness: [11.6046905]\n",
      "Generation 379/500, Best Fitness: [11.6046905]\n",
      "Generation 380/500, Best Fitness: [11.6046905]\n",
      "Generation 381/500, Best Fitness: [11.6046905]\n",
      "Generation 382/500, Best Fitness: [11.6046905]\n",
      "Generation 383/500, Best Fitness: [11.6046905]\n",
      "Generation 384/500, Best Fitness: [11.6046905]\n",
      "Generation 385/500, Best Fitness: [11.6046905]\n",
      "Generation 386/500, Best Fitness: [11.6046905]\n",
      "Generation 387/500, Best Fitness: [11.6046905]\n",
      "Generation 388/500, Best Fitness: [11.6046905]\n",
      "Generation 389/500, Best Fitness: [11.6046905]\n",
      "Generation 390/500, Best Fitness: [11.6046905]\n",
      "Generation 391/500, Best Fitness: [11.6046905]\n",
      "Generation 392/500, Best Fitness: [11.6046905]\n",
      "Generation 393/500, Best Fitness: [11.6046905]\n",
      "Generation 394/500, Best Fitness: [11.6046905]\n",
      "Generation 395/500, Best Fitness: [11.6046905]\n",
      "Generation 396/500, Best Fitness: [11.6046905]\n",
      "Generation 397/500, Best Fitness: [11.6046905]\n",
      "Generation 398/500, Best Fitness: [11.6046905]\n",
      "Generation 399/500, Best Fitness: [11.6046905]\n",
      "Generation 400/500, Best Fitness: [11.6046905]\n",
      "Generation 401/500, Best Fitness: [11.6046905]\n",
      "Generation 402/500, Best Fitness: [11.6046905]\n",
      "Generation 403/500, Best Fitness: [11.6046905]\n",
      "Generation 404/500, Best Fitness: [11.7269901]\n",
      "Generation 405/500, Best Fitness: [11.73055753]\n",
      "Generation 406/500, Best Fitness: [11.73055753]\n",
      "Generation 407/500, Best Fitness: [11.73245525]\n",
      "Generation 408/500, Best Fitness: [11.73256626]\n",
      "Generation 409/500, Best Fitness: [11.7330721]\n",
      "Generation 410/500, Best Fitness: [11.7330721]\n",
      "Generation 411/500, Best Fitness: [11.73358827]\n",
      "Generation 412/500, Best Fitness: [11.73358827]\n",
      "Generation 413/500, Best Fitness: [11.73358827]\n",
      "Generation 414/500, Best Fitness: [11.73421069]\n",
      "Generation 415/500, Best Fitness: [11.73421069]\n",
      "Generation 416/500, Best Fitness: [11.73421069]\n",
      "Generation 417/500, Best Fitness: [12.43231525]\n",
      "Generation 418/500, Best Fitness: [12.43231525]\n",
      "Generation 419/500, Best Fitness: [12.43231525]\n",
      "Generation 420/500, Best Fitness: [12.43231525]\n",
      "Generation 421/500, Best Fitness: [12.43231525]\n",
      "Generation 422/500, Best Fitness: [12.43231525]\n",
      "Generation 423/500, Best Fitness: [12.43231525]\n",
      "Generation 424/500, Best Fitness: [12.43231525]\n",
      "Generation 425/500, Best Fitness: [12.43231525]\n",
      "Generation 426/500, Best Fitness: [12.43231525]\n",
      "Generation 427/500, Best Fitness: [12.43231525]\n",
      "Generation 428/500, Best Fitness: [12.43231525]\n",
      "Generation 429/500, Best Fitness: [12.43231525]\n",
      "Generation 430/500, Best Fitness: [12.43231525]\n",
      "Generation 431/500, Best Fitness: [12.43231525]\n",
      "Generation 432/500, Best Fitness: [12.43231525]\n",
      "Generation 433/500, Best Fitness: [12.43231525]\n",
      "Generation 434/500, Best Fitness: [12.43231525]\n",
      "Generation 435/500, Best Fitness: [12.43231525]\n",
      "Generation 436/500, Best Fitness: [12.43231525]\n",
      "Generation 437/500, Best Fitness: [12.43231525]\n",
      "Generation 438/500, Best Fitness: [12.43231525]\n",
      "Generation 439/500, Best Fitness: [12.43231525]\n",
      "Generation 440/500, Best Fitness: [12.43231525]\n",
      "Generation 441/500, Best Fitness: [12.43231525]\n",
      "Generation 442/500, Best Fitness: [12.43231525]\n",
      "Generation 443/500, Best Fitness: [12.43231525]\n",
      "Generation 444/500, Best Fitness: [12.43231525]\n",
      "Generation 445/500, Best Fitness: [12.43231525]\n",
      "Generation 446/500, Best Fitness: [12.43231525]\n",
      "Generation 447/500, Best Fitness: [12.43231525]\n",
      "Generation 448/500, Best Fitness: [12.43231525]\n",
      "Generation 449/500, Best Fitness: [12.43231525]\n",
      "Generation 450/500, Best Fitness: [12.43231525]\n",
      "Generation 451/500, Best Fitness: [12.43231525]\n",
      "Generation 452/500, Best Fitness: [12.43231525]\n",
      "Generation 453/500, Best Fitness: [12.43231525]\n",
      "Generation 454/500, Best Fitness: [12.43231525]\n",
      "Generation 455/500, Best Fitness: [12.43231525]\n",
      "Generation 456/500, Best Fitness: [12.43231525]\n",
      "Generation 457/500, Best Fitness: [12.43231525]\n",
      "Generation 458/500, Best Fitness: [12.43231525]\n",
      "Generation 459/500, Best Fitness: [12.43231525]\n",
      "Generation 460/500, Best Fitness: [12.43231525]\n",
      "Generation 461/500, Best Fitness: [12.43231525]\n",
      "Generation 462/500, Best Fitness: [12.43231525]\n",
      "Generation 463/500, Best Fitness: [12.43231525]\n",
      "Generation 464/500, Best Fitness: [12.43231525]\n",
      "Generation 465/500, Best Fitness: [12.43231525]\n",
      "Generation 466/500, Best Fitness: [12.43231525]\n",
      "Generation 467/500, Best Fitness: [12.43231525]\n",
      "Generation 468/500, Best Fitness: [12.43231525]\n",
      "Generation 469/500, Best Fitness: [12.43231525]\n",
      "Generation 470/500, Best Fitness: [12.43231525]\n",
      "Generation 471/500, Best Fitness: [12.43231525]\n",
      "Generation 472/500, Best Fitness: [12.43231525]\n",
      "Generation 473/500, Best Fitness: [12.43231525]\n",
      "Generation 474/500, Best Fitness: [12.43231525]\n",
      "Generation 475/500, Best Fitness: [12.43231525]\n",
      "Generation 476/500, Best Fitness: [12.43231525]\n",
      "Generation 477/500, Best Fitness: [12.43231525]\n",
      "Generation 478/500, Best Fitness: [12.43231525]\n",
      "Generation 479/500, Best Fitness: [12.43231525]\n",
      "Generation 480/500, Best Fitness: [12.43231525]\n",
      "Generation 481/500, Best Fitness: [12.43231525]\n",
      "Generation 482/500, Best Fitness: [12.43231525]\n",
      "Generation 483/500, Best Fitness: [12.43231525]\n",
      "Generation 484/500, Best Fitness: [12.43231525]\n",
      "Generation 485/500, Best Fitness: [12.43231525]\n",
      "Generation 486/500, Best Fitness: [12.43231525]\n",
      "Generation 487/500, Best Fitness: [12.43231525]\n",
      "Generation 488/500, Best Fitness: [12.43231525]\n",
      "Generation 489/500, Best Fitness: [12.43231525]\n",
      "Generation 490/500, Best Fitness: [12.43231525]\n",
      "Generation 491/500, Best Fitness: [12.43231525]\n",
      "Generation 492/500, Best Fitness: [12.43231525]\n",
      "Generation 493/500, Best Fitness: [12.43231525]\n",
      "Generation 494/500, Best Fitness: [12.43231525]\n",
      "Generation 495/500, Best Fitness: [12.43231525]\n",
      "Generation 496/500, Best Fitness: [12.43231525]\n",
      "Generation 497/500, Best Fitness: [12.43231525]\n",
      "Generation 498/500, Best Fitness: [12.43231525]\n",
      "Generation 499/500, Best Fitness: [12.43231525]\n",
      "Generation 500/500, Best Fitness: [12.43231525]\n",
      "Best Individual: [[ 0.63201419+0.54291417j -0.58567954+0.25836137j -0.19277732-0.26827465j\n",
      "   0.10782532+0.15010443j  0.60484563-0.34484008j  0.38254825-0.41314679j\n",
      "   0.49389079+0.09382537j -0.23478887+0.66675473j  0.79457401+0.07438471j\n",
      "  -0.11007701-0.16574661j -0.91451782-0.10714493j  0.00692364+0.57639761j\n",
      "  -0.70804139-0.0609667j   0.46303868-0.56315915j  0.91832369+0.06602624j\n",
      "  -0.39896001+0.37350666j  0.01623412-0.52101089j  0.19486089-0.63609192j\n",
      "  -0.32264172+0.80438775j  0.79578544-0.09645999j -0.08567743-0.20023166j\n",
      "  -0.47677235+0.40831635j -0.03202919-0.5856578j   0.32643486-0.33629346j\n",
      "   0.47325933-0.19944914j  0.71190452-0.16501109j  0.179931  -0.82594559j\n",
      "   0.12892084-0.04665055j  0.0910825 +0.77158239j  0.23850285+0.82891658j\n",
      "   0.07013678+0.24811403j  0.25414272-0.1800475j ]\n",
      " [ 0.6335953 -0.12277313j -0.20468941+0.13635294j  0.14622993-0.09469004j\n",
      "   0.19961257-0.73643135j  0.27052617+0.40962737j -0.29930354-0.15545226j\n",
      "  -0.01667952-0.64265264j -0.16299327+0.47228609j -0.45227341+0.5595805j\n",
      "  -0.49843539-0.39780787j -0.18409597+0.08543011j  0.36397201+0.0250435j\n",
      "   0.56449406+0.56738607j  0.65824837-0.46521236j  0.22934162+0.57628801j\n",
      "  -0.58824102-0.18604931j  0.4296858 +0.48987403j  0.68459635+0.12552468j\n",
      "  -0.62209678+0.27015807j  0.66873276+0.13021888j -0.17256174-0.28460282j\n",
      "   0.63279898+0.53763165j  0.53550453-0.68746609j -0.45267147+0.10209483j\n",
      "  -0.54836332-0.31655156j -0.57952303-0.55274852j -0.17024212-0.56770195j\n",
      "   0.38600456+0.3884046j  -0.33573681-0.17802795j -0.06584093-0.72762463j\n",
      "   0.08800989-0.65884617j -0.27996268+0.06998586j]\n",
      " [ 0.17767853+0.61009487j  0.19498321+0.06944231j  0.20548937-0.01818448j\n",
      "  -0.22230528-0.16837308j  0.39687072+0.2635944j  -0.76430826-0.1071008j\n",
      "  -0.61148097+0.18314666j -0.12128317-0.70234672j -0.56829921-0.26068252j\n",
      "  -0.11175371+0.12445815j -0.62533784+0.26949114j  0.01956649+0.16022386j\n",
      "  -0.27379427+0.28767119j -0.32183577-0.11274506j  0.05137603-0.25620332j\n",
      "   0.24358187-0.25324953j -0.36011131-0.07865964j  0.2693363 -0.74529784j\n",
      "  -0.10640213-0.62333736j  0.20285648-0.78158534j  0.03288691+0.14454069j\n",
      "  -0.00915081+0.5733381j  -0.07830086+0.81107276j -0.10644735-0.76487797j\n",
      "   0.24163787-0.27959315j -0.05127484-0.5111655j  -0.79588944-0.40170834j\n",
      "  -0.17431565-0.40621587j -0.55873278-0.77706412j -0.01469523-0.22409355j\n",
      "   0.06757006-0.04163368j -0.62857428-0.42460895j]\n",
      " [ 0.41383596-0.32699809j  0.35258428-0.26520332j  0.19364926+0.28016073j\n",
      "   0.34312872-0.70755668j  0.16950205-0.33997492j  0.01179036+0.50652864j\n",
      "   0.28171916+0.58513449j  0.68546025+0.61485499j -0.20941992+0.4761538j\n",
      "  -0.0443411 +0.14164101j -0.75263806-0.06085335j -0.0076349 -0.23234477j\n",
      "   0.17627885+0.52094208j  0.20123634-0.4675636j   0.26161616+0.38521015j\n",
      "   0.19775117-0.27059146j  0.38356998+0.0081024j  -0.60485844-0.02181976j\n",
      "   0.04691444+0.75681076j  0.63630706+0.14296956j -0.70529259-0.24351269j\n",
      "   0.29225531+0.00431673j -0.38450501-0.59144964j  0.74444233-0.13182302j\n",
      "  -0.83721976-0.04560191j  0.14555995-0.21910491j  0.0391832 -0.75045807j\n",
      "   0.60614951-0.1174877j   0.83603133-0.45156472j  0.69795523-0.03391916j\n",
      "  -0.70137993-0.24951388j  0.70235906+0.27720082j]] [[-0.12802439+0.10527367j -0.43593275-0.09133007j  0.02238257-0.12029207j\n",
      "  -0.02765139+0.01614733j -0.25405237-0.13230192j -0.18021103-0.01475239j\n",
      "   0.07763717+0.15763256j  0.03689885-0.04728044j]\n",
      " [-0.2489778 -0.13578466j  0.14449939+0.07106871j  0.19783918+0.01619271j\n",
      "  -0.0344351 +0.25068175j -0.05546886+0.14580195j -0.09315605+0.22823751j\n",
      "  -0.18100306+0.08688817j  0.16510535+0.36275134j]\n",
      " [-0.39262816-0.26518495j  0.19925322+0.04040909j -0.06130363-0.00986449j\n",
      "  -0.24534727+0.24683093j  0.26522371+0.18781497j -0.384626  -0.13229596j\n",
      "   0.3809152 +0.382553j   -0.21258241+0.03895074j]\n",
      " [-0.04290124-0.01963423j -0.05008755+0.13950721j  0.21172598-0.09861626j\n",
      "  -0.03003497-0.34151769j  0.08611631+0.11498181j -0.1507199 +0.01977207j\n",
      "  -0.05194602+0.23154479j -0.01234291+0.23412574j]]\n",
      "Best Fitness: [12.43231525]\n"
     ]
    }
   ],
   "source": [
    "# Run the genetic algorithm\n",
    "best_individual = genetic_algorithm()\n",
    "print(\"Best Individual:\", best_individual.theta, best_individual.w)\n",
    "print(\"Best Fitness:\", secrecy_rate_objective_function(best_individual.theta, best_individual.w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of PSO and GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSO_GD(w_PSO_GD, max_iter=100):\n",
    "    particles = [Particle() for _ in range (number_of_users)]\n",
    "    gbest_theta = particles[0].theta.copy()\n",
    "    gbest_value = particles[0].pbest_value.copy()\n",
    "    w_max = 0.9\n",
    "    w_min = 0.4\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        E_t = float((max_iter - iteration - 1)/max_iter)\n",
    "        inertia = w_min + (w_max - w_min) * (2 /(1 + (np.e ** (-5 * E_t))) - 1)\n",
    "\n",
    "        \n",
    "        for k in range(number_of_users):\n",
    "            particles[k].update_velocity_theta(gbest_theta, inertia, user_k=k)\n",
    "            particles[k].update_position_theta(user_k=k)\n",
    "            \n",
    "            fitness_value = secrecy_rate_objective_function(particles[k].theta, particles[k].w)\n",
    "            \n",
    "            if fitness_value > particles[k].pbest_value:\n",
    "                particles[k].pbest_theta = particles[k].theta.copy()\n",
    "                particles[k].pbest_value = fitness_value\n",
    "            \n",
    "            if fitness_value > gbest_value:\n",
    "                gbest_theta = particles[k].theta.copy()\n",
    "                gbest_value = fitness_value\n",
    "            \n",
    "        # print(f\"Iteration {iteration+1}/{total_iter}, Global Best Value: {gbest_value}\\n\")\n",
    "    \n",
    "    print(\"Global Best Value Before Gradient Descent: \", secrecy_rate_objective_function(gbest_theta, w_PSO_GD))\n",
    "    \n",
    "    gbest_w = gradient_descent_w(gbest_theta, w_PSO_GD, learning_rate=0.01, total_iter=500)\n",
    "    print(\"Global Best Value: \", secrecy_rate_objective_function(gbest_theta, gbest_w))\n",
    "    \n",
    "    return gbest_theta, gbest_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Secrecy Rate PSO-GD: [2.02873527]\n",
      "Global Best Value Before Gradient Descent:  [2.03029224]\n",
      "Global Best Value:  [25.19064664]\n",
      "Cycle: 1 Objective Function Value: [25.19064664]\n",
      "Global Best Value Before Gradient Descent:  [25.21230855]\n",
      "Global Best Value:  [25.21230855]\n",
      "Cycle: 2 Objective Function Value: [25.21230855]\n",
      "Global Best Value Before Gradient Descent:  [25.23006129]\n",
      "Global Best Value:  [25.23006129]\n",
      "Cycle: 3 Objective Function Value: [25.23006129]\n",
      "Global Best Value Before Gradient Descent:  [25.24354691]\n",
      "Stopped at iteration 162\n",
      "0.001 -3.4940658803606084e-05\n",
      "Global Best Value:  [25.24354691]\n",
      "Cycle: 4 Objective Function Value: [25.24354691]\n",
      "Global Best Value Before Gradient Descent:  [25.24913185]\n",
      "Global Best Value:  [25.24913185]\n",
      "Cycle: 5 Objective Function Value: [25.24913185]\n",
      "Global Best Value Before Gradient Descent:  [25.25473558]\n",
      "Global Best Value:  [25.25473558]\n",
      "Cycle: 6 Objective Function Value: [25.25473558]\n",
      "Global Best Value Before Gradient Descent:  [25.25991649]\n",
      "Global Best Value:  [25.25991649]\n",
      "Cycle: 7 Objective Function Value: [25.25991649]\n",
      "Global Best Value Before Gradient Descent:  [25.26327262]\n",
      "Global Best Value:  [25.26327262]\n",
      "Cycle: 8 Objective Function Value: [25.26327262]\n",
      "Global Best Value Before Gradient Descent:  [25.26636447]\n",
      "Global Best Value:  [25.39127343]\n",
      "Cycle: 9 Objective Function Value: [25.39127343]\n",
      "Global Best Value Before Gradient Descent:  [25.39448483]\n",
      "Global Best Value:  [25.39448483]\n",
      "Cycle: 10 Objective Function Value: [25.39448483]\n",
      "Global Best Value Before Gradient Descent:  [25.40780906]\n",
      "Global Best Value:  [25.42913935]\n",
      "Cycle: 11 Objective Function Value: [25.42913935]\n",
      "Global Best Value Before Gradient Descent:  [25.43207978]\n",
      "Global Best Value:  [25.43207978]\n",
      "Cycle: 12 Objective Function Value: [25.43207978]\n",
      "Global Best Value Before Gradient Descent:  [25.4377401]\n",
      "Global Best Value:  [25.4377401]\n",
      "Cycle: 13 Objective Function Value: [25.4377401]\n",
      "Global Best Value Before Gradient Descent:  [25.4398477]\n",
      "Global Best Value:  [25.4398477]\n",
      "Cycle: 14 Objective Function Value: [25.4398477]\n",
      "Global Best Value Before Gradient Descent:  [25.4460342]\n",
      "Global Best Value:  [25.4460342]\n",
      "Cycle: 15 Objective Function Value: [25.4460342]\n",
      "Global Best Value Before Gradient Descent:  [25.44923818]\n",
      "Global Best Value:  [25.44923818]\n",
      "Cycle: 16 Objective Function Value: [25.44923818]\n",
      "Global Best Value Before Gradient Descent:  [25.45103801]\n",
      "Global Best Value:  [25.45103801]\n",
      "Cycle: 17 Objective Function Value: [25.45103801]\n",
      "Global Best Value Before Gradient Descent:  [25.45394555]\n",
      "Global Best Value:  [25.45394555]\n",
      "Cycle: 18 Objective Function Value: [25.45394555]\n",
      "Global Best Value Before Gradient Descent:  [25.45691174]\n",
      "Global Best Value:  [25.45691174]\n",
      "Cycle: 19 Objective Function Value: [25.45691174]\n",
      "Global Best Value Before Gradient Descent:  [25.45913827]\n",
      "Global Best Value:  [25.45913827]\n",
      "Cycle: 20 Objective Function Value: [25.45913827]\n",
      "Global Best Value Before Gradient Descent:  [25.46236735]\n",
      "Global Best Value:  [25.46236735]\n",
      "Cycle: 21 Objective Function Value: [25.46236735]\n",
      "Global Best Value Before Gradient Descent:  [25.46593479]\n",
      "Global Best Value:  [25.46593479]\n",
      "Cycle: 22 Objective Function Value: [25.46593479]\n",
      "Global Best Value Before Gradient Descent:  [25.46765448]\n",
      "Global Best Value:  [25.46765448]\n",
      "Cycle: 23 Objective Function Value: [25.46765448]\n",
      "Global Best Value Before Gradient Descent:  [25.47041196]\n",
      "Global Best Value:  [25.47041196]\n",
      "Cycle: 24 Objective Function Value: [25.47041196]\n",
      "Global Best Value Before Gradient Descent:  [25.47217405]\n",
      "Global Best Value:  [25.47217405]\n",
      "Cycle: 25 Objective Function Value: [25.47217405]\n",
      "Global Best Value Before Gradient Descent:  [25.47395456]\n",
      "Global Best Value:  [25.47395456]\n",
      "Cycle: 26 Objective Function Value: [25.47395456]\n",
      "Global Best Value Before Gradient Descent:  [25.4747304]\n",
      "Stopped at iteration 230\n",
      "0.001 0.00018635952883627738\n",
      "Global Best Value:  [25.4747304]\n",
      "Cycle: 27 Objective Function Value: [25.4747304]\n",
      "Global Best Value Before Gradient Descent:  [25.47601757]\n",
      "Global Best Value:  [25.47601757]\n",
      "Cycle: 28 Objective Function Value: [25.47601757]\n",
      "Global Best Value Before Gradient Descent:  [25.47717417]\n",
      "Global Best Value:  [25.47717417]\n",
      "Cycle: 29 Objective Function Value: [25.47717417]\n",
      "Global Best Value Before Gradient Descent:  [25.47803188]\n",
      "Global Best Value:  [25.47803188]\n",
      "Cycle: 30 Objective Function Value: [25.47803188]\n",
      "Global Best Value Before Gradient Descent:  [25.47865817]\n",
      "Global Best Value:  [25.47865817]\n",
      "Cycle: 31 Objective Function Value: [25.47865817]\n",
      "Global Best Value Before Gradient Descent:  [25.47961609]\n",
      "Global Best Value:  [25.47961609]\n",
      "Cycle: 32 Objective Function Value: [25.47961609]\n",
      "Global Best Value Before Gradient Descent:  [25.48029765]\n",
      "Global Best Value:  [25.48029765]\n",
      "Cycle: 33 Objective Function Value: [25.48029765]\n",
      "Global Best Value Before Gradient Descent:  [25.48171175]\n",
      "Global Best Value:  [25.48171175]\n",
      "Cycle: 34 Objective Function Value: [25.48171175]\n",
      "Global Best Value Before Gradient Descent:  [25.48199727]\n",
      "Global Best Value:  [25.48199727]\n",
      "Cycle: 35 Objective Function Value: [25.48199727]\n",
      "Global Best Value Before Gradient Descent:  [25.48298212]\n",
      "Global Best Value:  [25.48298212]\n",
      "Cycle: 36 Objective Function Value: [25.48298212]\n",
      "Global Best Value Before Gradient Descent:  [25.48360458]\n",
      "Global Best Value:  [25.48360458]\n",
      "Cycle: 37 Objective Function Value: [25.48360458]\n",
      "Global Best Value Before Gradient Descent:  [25.48386592]\n",
      "Global Best Value:  [25.48386592]\n",
      "Cycle: 38 Objective Function Value: [25.48386592]\n",
      "Global Best Value Before Gradient Descent:  [25.48508007]\n",
      "Global Best Value:  [25.48508007]\n",
      "Cycle: 39 Objective Function Value: [25.48508007]\n",
      "Global Best Value Before Gradient Descent:  [25.48567246]\n",
      "Stopped at iteration 454\n",
      "0.001 0.0008142089651741458\n",
      "Global Best Value:  [25.48567246]\n",
      "Cycle: 40 Objective Function Value: [25.48567246]\n",
      "Global Best Value Before Gradient Descent:  [25.48684402]\n",
      "Global Best Value:  [25.48684402]\n",
      "Cycle: 41 Objective Function Value: [25.48684402]\n",
      "Global Best Value Before Gradient Descent:  [25.48772795]\n",
      "Global Best Value:  [25.48772795]\n",
      "Cycle: 42 Objective Function Value: [25.48772795]\n",
      "Global Best Value Before Gradient Descent:  [25.48824161]\n",
      "Global Best Value:  [25.48824161]\n",
      "Cycle: 43 Objective Function Value: [25.48824161]\n",
      "Global Best Value Before Gradient Descent:  [25.48862738]\n",
      "Global Best Value:  [25.48862738]\n",
      "Cycle: 44 Objective Function Value: [25.48862738]\n",
      "Global Best Value Before Gradient Descent:  [25.48931997]\n",
      "Global Best Value:  [25.48931997]\n",
      "Cycle: 45 Objective Function Value: [25.48931997]\n",
      "Global Best Value Before Gradient Descent:  [25.48978124]\n",
      "Global Best Value:  [25.48978124]\n",
      "Cycle: 46 Objective Function Value: [25.48978124]\n",
      "Global Best Value Before Gradient Descent:  [25.49018911]\n",
      "Global Best Value:  [25.49018911]\n",
      "Cycle: 47 Objective Function Value: [25.49018911]\n",
      "Global Best Value Before Gradient Descent:  [25.49043415]\n",
      "Global Best Value:  [25.49043415]\n",
      "Cycle: 48 Objective Function Value: [25.49043415]\n",
      "Global Best Value Before Gradient Descent:  [25.49097864]\n",
      "Global Best Value:  [25.49097864]\n",
      "Cycle: 49 Objective Function Value: [25.49097864]\n",
      "Global Best Value Before Gradient Descent:  [25.49115711]\n",
      "Global Best Value:  [25.49115711]\n",
      "Cycle: 50 Objective Function Value: [25.49115711]\n",
      "Global Best Value Before Gradient Descent:  [25.49180513]\n",
      "Global Best Value:  [25.49180513]\n",
      "Cycle: 51 Objective Function Value: [25.49180513]\n",
      "Global Best Value Before Gradient Descent:  [25.49227999]\n",
      "Global Best Value:  [25.49227999]\n",
      "Cycle: 52 Objective Function Value: [25.49227999]\n",
      "Global Best Value Before Gradient Descent:  [25.49287007]\n",
      "Global Best Value:  [25.49287007]\n",
      "Cycle: 53 Objective Function Value: [25.49287007]\n",
      "Global Best Value Before Gradient Descent:  [25.49330841]\n",
      "Global Best Value:  [25.49330841]\n",
      "Cycle: 54 Objective Function Value: [25.49330841]\n",
      "Global Best Value Before Gradient Descent:  [25.49397413]\n",
      "Global Best Value:  [25.49397413]\n",
      "Cycle: 55 Objective Function Value: [25.49397413]\n",
      "Global Best Value Before Gradient Descent:  [25.49461039]\n",
      "Global Best Value:  [25.49461039]\n",
      "Cycle: 56 Objective Function Value: [25.49461039]\n",
      "Global Best Value Before Gradient Descent:  [25.49510279]\n",
      "Global Best Value:  [25.49510279]\n",
      "Cycle: 57 Objective Function Value: [25.49510279]\n",
      "Global Best Value Before Gradient Descent:  [25.49547745]\n",
      "Global Best Value:  [25.49547745]\n",
      "Cycle: 58 Objective Function Value: [25.49547745]\n",
      "Global Best Value Before Gradient Descent:  [25.49586808]\n",
      "Global Best Value:  [25.49586808]\n",
      "Cycle: 59 Objective Function Value: [25.49586808]\n",
      "Global Best Value Before Gradient Descent:  [25.49625312]\n",
      "Global Best Value:  [25.49625312]\n",
      "Cycle: 60 Objective Function Value: [25.49625312]\n",
      "Global Best Value Before Gradient Descent:  [25.49690962]\n",
      "Global Best Value:  [25.49690962]\n",
      "Cycle: 61 Objective Function Value: [25.49690962]\n",
      "Global Best Value Before Gradient Descent:  [25.49704609]\n",
      "Global Best Value:  [25.49704609]\n",
      "Cycle: 62 Objective Function Value: [25.49704609]\n",
      "Global Best Value Before Gradient Descent:  [25.49763838]\n",
      "Global Best Value:  [25.49763838]\n",
      "Cycle: 63 Objective Function Value: [25.49763838]\n",
      "Global Best Value Before Gradient Descent:  [25.49813764]\n",
      "Global Best Value:  [25.49813764]\n",
      "Cycle: 64 Objective Function Value: [25.49813764]\n",
      "Global Best Value Before Gradient Descent:  [25.49895363]\n",
      "Global Best Value:  [25.49895363]\n",
      "Cycle: 65 Objective Function Value: [25.49895363]\n",
      "Global Best Value Before Gradient Descent:  [25.49935616]\n",
      "Stopped at iteration 460\n",
      "0.001 -0.0006265328990515684\n",
      "Global Best Value:  [25.49935616]\n",
      "Cycle: 66 Objective Function Value: [25.49935616]\n",
      "Global Best Value Before Gradient Descent:  [25.49992382]\n",
      "Global Best Value:  [25.49992382]\n",
      "Cycle: 67 Objective Function Value: [25.49992382]\n",
      "Global Best Value Before Gradient Descent:  [25.50012465]\n",
      "Global Best Value:  [25.50012465]\n",
      "Cycle: 68 Objective Function Value: [25.50012465]\n",
      "Global Best Value Before Gradient Descent:  [25.50032027]\n",
      "Global Best Value:  [25.50032027]\n",
      "Cycle: 69 Objective Function Value: [25.50032027]\n",
      "Global Best Value Before Gradient Descent:  [25.50075091]\n",
      "Global Best Value:  [25.50075091]\n",
      "Cycle: 70 Objective Function Value: [25.50075091]\n",
      "Global Best Value Before Gradient Descent:  [25.50092586]\n",
      "Global Best Value:  [25.50092586]\n",
      "Cycle: 71 Objective Function Value: [25.50092586]\n",
      "Global Best Value Before Gradient Descent:  [25.50101788]\n",
      "Global Best Value:  [25.50101788]\n",
      "Cycle: 72 Objective Function Value: [25.50101788]\n",
      "Global Best Value Before Gradient Descent:  [25.50126038]\n",
      "Global Best Value:  [25.50126038]\n",
      "Cycle: 73 Objective Function Value: [25.50126038]\n",
      "Global Best Value Before Gradient Descent:  [25.50159423]\n",
      "Global Best Value:  [25.50159423]\n",
      "Cycle: 74 Objective Function Value: [25.50159423]\n",
      "Global Best Value Before Gradient Descent:  [25.50171698]\n",
      "Global Best Value:  [25.50171698]\n",
      "Cycle: 75 Objective Function Value: [25.50171698]\n",
      "Global Best Value Before Gradient Descent:  [25.50205013]\n",
      "Global Best Value:  [25.50205013]\n",
      "Cycle: 76 Objective Function Value: [25.50205013]\n",
      "Global Best Value Before Gradient Descent:  [25.50223636]\n",
      "Global Best Value:  [25.50223636]\n",
      "Cycle: 77 Objective Function Value: [25.50223636]\n",
      "Global Best Value Before Gradient Descent:  [25.50267166]\n",
      "Global Best Value:  [25.50267166]\n",
      "Cycle: 78 Objective Function Value: [25.50267166]\n",
      "Global Best Value Before Gradient Descent:  [25.50283265]\n",
      "Global Best Value:  [25.50283265]\n",
      "Cycle: 79 Objective Function Value: [25.50283265]\n",
      "Global Best Value Before Gradient Descent:  [25.50295389]\n",
      "Global Best Value:  [25.50295389]\n",
      "Cycle: 80 Objective Function Value: [25.50295389]\n",
      "Global Best Value Before Gradient Descent:  [25.50329828]\n",
      "Global Best Value:  [25.50329828]\n",
      "Cycle: 81 Objective Function Value: [25.50329828]\n",
      "Global Best Value Before Gradient Descent:  [25.50347436]\n",
      "Global Best Value:  [25.50347436]\n",
      "Cycle: 82 Objective Function Value: [25.50347436]\n",
      "Global Best Value Before Gradient Descent:  [25.50371649]\n",
      "Global Best Value:  [25.50371649]\n",
      "Cycle: 83 Objective Function Value: [25.50371649]\n",
      "Global Best Value Before Gradient Descent:  [25.50399673]\n",
      "Global Best Value:  [25.50399673]\n",
      "Cycle: 84 Objective Function Value: [25.50399673]\n",
      "Global Best Value Before Gradient Descent:  [25.50412751]\n",
      "Global Best Value:  [25.50412751]\n",
      "Cycle: 85 Objective Function Value: [25.50412751]\n",
      "Global Best Value Before Gradient Descent:  [25.5042104]\n",
      "Global Best Value:  [25.5042104]\n",
      "Cycle: 86 Objective Function Value: [25.5042104]\n",
      "Global Best Value Before Gradient Descent:  [25.50483927]\n",
      "Global Best Value:  [25.50483927]\n",
      "Cycle: 87 Objective Function Value: [25.50483927]\n",
      "Global Best Value Before Gradient Descent:  [25.50494286]\n",
      "Global Best Value:  [25.50494286]\n",
      "Cycle: 88 Objective Function Value: [25.50494286]\n",
      "Global Best Value Before Gradient Descent:  [25.50509982]\n",
      "Global Best Value:  [25.50509982]\n",
      "Cycle: 89 Objective Function Value: [25.50509982]\n",
      "Global Best Value Before Gradient Descent:  [25.50522707]\n",
      "Global Best Value:  [25.50522707]\n",
      "Cycle: 90 Objective Function Value: [25.50522707]\n",
      "Global Best Value Before Gradient Descent:  [25.5053126]\n",
      "Global Best Value:  [25.5053126]\n",
      "Cycle: 91 Objective Function Value: [25.5053126]\n",
      "Global Best Value Before Gradient Descent:  [25.50550434]\n",
      "Global Best Value:  [25.50550434]\n",
      "Cycle: 92 Objective Function Value: [25.50550434]\n",
      "Global Best Value Before Gradient Descent:  [25.50558449]\n",
      "Global Best Value:  [25.50558449]\n",
      "Cycle: 93 Objective Function Value: [25.50558449]\n",
      "Global Best Value Before Gradient Descent:  [25.50581161]\n",
      "Stopped at iteration 490\n",
      "0.001 -0.0003417199756547973\n",
      "Global Best Value:  [25.50581161]\n",
      "Cycle: 94 Objective Function Value: [25.50581161]\n",
      "Global Best Value Before Gradient Descent:  [25.50600407]\n",
      "Global Best Value:  [25.50600407]\n",
      "Cycle: 95 Objective Function Value: [25.50600407]\n",
      "Global Best Value Before Gradient Descent:  [25.50614255]\n",
      "Global Best Value:  [25.50614255]\n",
      "Cycle: 96 Objective Function Value: [25.50614255]\n",
      "Global Best Value Before Gradient Descent:  [25.50638402]\n",
      "Global Best Value:  [25.50638402]\n",
      "Cycle: 97 Objective Function Value: [25.50638402]\n",
      "Global Best Value Before Gradient Descent:  [25.50647044]\n",
      "Global Best Value:  [25.53407306]\n",
      "Cycle: 98 Objective Function Value: [25.53407306]\n",
      "Global Best Value Before Gradient Descent:  [25.53544901]\n",
      "Global Best Value:  [25.53544901]\n",
      "Cycle: 99 Objective Function Value: [25.53544901]\n",
      "Global Best Value Before Gradient Descent:  [25.53687637]\n",
      "Global Best Value:  [25.53687637]\n",
      "Cycle: 100 Objective Function Value: [25.53687637]\n",
      "Global Best Value Before Gradient Descent:  [25.5376809]\n",
      "Global Best Value:  [25.5376809]\n",
      "Cycle: 101 Objective Function Value: [25.5376809]\n",
      "Global Best Value Before Gradient Descent:  [25.53789808]\n",
      "Global Best Value:  [25.53789808]\n",
      "Cycle: 102 Objective Function Value: [25.53789808]\n",
      "Global Best Value Before Gradient Descent:  [25.53844641]\n",
      "Global Best Value:  [25.53844641]\n",
      "Cycle: 103 Objective Function Value: [25.53844641]\n",
      "Global Best Value Before Gradient Descent:  [25.5395115]\n",
      "Global Best Value:  [25.5395115]\n",
      "Cycle: 104 Objective Function Value: [25.5395115]\n",
      "Global Best Value Before Gradient Descent:  [25.53996373]\n",
      "Global Best Value:  [25.53996373]\n",
      "Cycle: 105 Objective Function Value: [25.53996373]\n",
      "Global Best Value Before Gradient Descent:  [25.5404615]\n",
      "Global Best Value:  [25.5404615]\n",
      "Cycle: 106 Objective Function Value: [25.5404615]\n",
      "Global Best Value Before Gradient Descent:  [25.54277369]\n",
      "Global Best Value:  [25.54277369]\n",
      "Cycle: 107 Objective Function Value: [25.54277369]\n",
      "Global Best Value Before Gradient Descent:  [25.54359297]\n",
      "Global Best Value:  [25.54359297]\n",
      "Cycle: 108 Objective Function Value: [25.54359297]\n",
      "Global Best Value Before Gradient Descent:  [25.54388841]\n",
      "Global Best Value:  [25.54388841]\n",
      "Cycle: 109 Objective Function Value: [25.54388841]\n",
      "Global Best Value Before Gradient Descent:  [25.54419669]\n",
      "Global Best Value:  [25.54419669]\n",
      "Cycle: 110 Objective Function Value: [25.54419669]\n",
      "Global Best Value Before Gradient Descent:  [25.54453522]\n",
      "Global Best Value:  [25.54453522]\n",
      "Cycle: 111 Objective Function Value: [25.54453522]\n",
      "Global Best Value Before Gradient Descent:  [25.54498542]\n",
      "Global Best Value:  [25.54498542]\n",
      "Cycle: 112 Objective Function Value: [25.54498542]\n",
      "Global Best Value Before Gradient Descent:  [25.54539893]\n",
      "Global Best Value:  [25.54539893]\n",
      "Cycle: 113 Objective Function Value: [25.54539893]\n",
      "Global Best Value Before Gradient Descent:  [25.54602807]\n",
      "Global Best Value:  [25.54602807]\n",
      "Cycle: 114 Objective Function Value: [25.54602807]\n",
      "Global Best Value Before Gradient Descent:  [25.54648622]\n",
      "Stopped at iteration 151\n",
      "0.001 -0.00040093617972303264\n",
      "Global Best Value:  [25.54648622]\n",
      "Cycle: 115 Objective Function Value: [25.54648622]\n",
      "Global Best Value Before Gradient Descent:  [25.54699629]\n",
      "Global Best Value:  [25.54699629]\n",
      "Cycle: 116 Objective Function Value: [25.54699629]\n",
      "Global Best Value Before Gradient Descent:  [25.54715279]\n",
      "Global Best Value:  [25.54715279]\n",
      "Cycle: 117 Objective Function Value: [25.54715279]\n",
      "Global Best Value Before Gradient Descent:  [25.54806281]\n",
      "Global Best Value:  [25.54806281]\n",
      "Cycle: 118 Objective Function Value: [25.54806281]\n",
      "Global Best Value Before Gradient Descent:  [25.54826085]\n",
      "Global Best Value:  [25.54826085]\n",
      "Cycle: 119 Objective Function Value: [25.54826085]\n",
      "Global Best Value Before Gradient Descent:  [25.54877309]\n",
      "Global Best Value:  [25.54877309]\n",
      "Cycle: 120 Objective Function Value: [25.54877309]\n",
      "Global Best Value Before Gradient Descent:  [25.54901807]\n",
      "Global Best Value:  [25.54901807]\n",
      "Cycle: 121 Objective Function Value: [25.54901807]\n",
      "Global Best Value Before Gradient Descent:  [25.54924195]\n",
      "Global Best Value:  [25.54924195]\n",
      "Cycle: 122 Objective Function Value: [25.54924195]\n",
      "Global Best Value Before Gradient Descent:  [25.54940524]\n",
      "Global Best Value:  [25.54940524]\n",
      "Cycle: 123 Objective Function Value: [25.54940524]\n",
      "Global Best Value Before Gradient Descent:  [25.54948286]\n",
      "Global Best Value:  [25.54948286]\n",
      "Cycle: 124 Objective Function Value: [25.54948286]\n",
      "Global Best Value Before Gradient Descent:  [25.54977114]\n",
      "Global Best Value:  [25.54977114]\n",
      "Cycle: 125 Objective Function Value: [25.54977114]\n",
      "Global Best Value Before Gradient Descent:  [25.5503274]\n",
      "Global Best Value:  [25.5503274]\n",
      "Cycle: 126 Objective Function Value: [25.5503274]\n",
      "Global Best Value Before Gradient Descent:  [25.55044781]\n",
      "Global Best Value:  [25.55044781]\n",
      "Cycle: 127 Objective Function Value: [25.55044781]\n",
      "Global Best Value Before Gradient Descent:  [25.55061991]\n",
      "Global Best Value:  [25.55061991]\n",
      "Cycle: 128 Objective Function Value: [25.55061991]\n",
      "Global Best Value Before Gradient Descent:  [25.55081711]\n",
      "Global Best Value:  [25.55081711]\n",
      "Cycle: 129 Objective Function Value: [25.55081711]\n",
      "Global Best Value Before Gradient Descent:  [25.55092599]\n",
      "Global Best Value:  [25.56450727]\n",
      "Cycle: 130 Objective Function Value: [25.56450727]\n",
      "Global Best Value Before Gradient Descent:  [25.5648705]\n",
      "Global Best Value:  [25.5648705]\n",
      "Cycle: 131 Objective Function Value: [25.5648705]\n",
      "Global Best Value Before Gradient Descent:  [25.56491977]\n",
      "Global Best Value:  [25.56491977]\n",
      "Cycle: 132 Objective Function Value: [25.56491977]\n",
      "Global Best Value Before Gradient Descent:  [25.56508115]\n",
      "Global Best Value:  [25.56508115]\n",
      "Cycle: 133 Objective Function Value: [25.56508115]\n",
      "Global Best Value Before Gradient Descent:  [25.56545565]\n",
      "Global Best Value:  [25.56545565]\n",
      "Cycle: 134 Objective Function Value: [25.56545565]\n",
      "Global Best Value Before Gradient Descent:  [25.56557571]\n",
      "Global Best Value:  [25.56557571]\n",
      "Cycle: 135 Objective Function Value: [25.56557571]\n",
      "Global Best Value Before Gradient Descent:  [25.56586598]\n",
      "Global Best Value:  [25.56586598]\n",
      "Cycle: 136 Objective Function Value: [25.56586598]\n",
      "Global Best Value Before Gradient Descent:  [25.56593916]\n",
      "Global Best Value:  [25.56593916]\n",
      "Cycle: 137 Objective Function Value: [25.56593916]\n",
      "Global Best Value Before Gradient Descent:  [25.56612967]\n",
      "Global Best Value:  [25.56612967]\n",
      "Cycle: 138 Objective Function Value: [25.56612967]\n",
      "Global Best Value Before Gradient Descent:  [25.56622801]\n",
      "Global Best Value:  [25.56622801]\n",
      "Cycle: 139 Objective Function Value: [25.56622801]\n",
      "Global Best Value Before Gradient Descent:  [25.56648211]\n",
      "Global Best Value:  [25.56648211]\n",
      "Cycle: 140 Objective Function Value: [25.56648211]\n",
      "Global Best Value Before Gradient Descent:  [25.56651904]\n",
      "Global Best Value:  [25.56651904]\n",
      "Cycle: 141 Objective Function Value: [25.56651904]\n",
      "Global Best Value Before Gradient Descent:  [25.56673797]\n",
      "Global Best Value:  [25.56673797]\n",
      "Cycle: 142 Objective Function Value: [25.56673797]\n",
      "Global Best Value Before Gradient Descent:  [25.56689187]\n",
      "Global Best Value:  [25.56689187]\n",
      "Cycle: 143 Objective Function Value: [25.56689187]\n",
      "Global Best Value Before Gradient Descent:  [25.56697555]\n",
      "Global Best Value:  [25.56697555]\n",
      "Cycle: 144 Objective Function Value: [25.56697555]\n",
      "Global Best Value Before Gradient Descent:  [25.56719226]\n",
      "Global Best Value:  [25.56719226]\n",
      "Cycle: 145 Objective Function Value: [25.56719226]\n",
      "Global Best Value Before Gradient Descent:  [25.56734598]\n",
      "Global Best Value:  [25.56734598]\n",
      "Cycle: 146 Objective Function Value: [25.56734598]\n",
      "Global Best Value Before Gradient Descent:  [25.5673867]\n",
      "Global Best Value:  [25.5673867]\n",
      "Cycle: 147 Objective Function Value: [25.5673867]\n",
      "Global Best Value Before Gradient Descent:  [25.56760898]\n",
      "Global Best Value:  [25.56760898]\n",
      "Cycle: 148 Objective Function Value: [25.56760898]\n",
      "Global Best Value Before Gradient Descent:  [25.56773208]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m num_cycles \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_cycles):\n\u001b[0;32m---> 13\u001b[0m     theta_opt, w_opt \u001b[38;5;241m=\u001b[39m \u001b[43mPSO_GD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_PSO\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     theta_PSO, w_PSO \u001b[38;5;241m=\u001b[39m theta_opt, w_opt\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCycle:\u001b[39m\u001b[38;5;124m\"\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObjective Function Value:\u001b[39m\u001b[38;5;124m\"\u001b[39m, secrecy_rate_objective_function(theta_opt, w_opt))\n",
      "Cell \u001b[0;32mIn[21], line 31\u001b[0m, in \u001b[0;36mPSO_GD\u001b[0;34m(w_PSO_GD, max_iter)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# print(f\"Iteration {iteration+1}/{total_iter}, Global Best Value: {gbest_value}\\n\")\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal Best Value Before Gradient Descent: \u001b[39m\u001b[38;5;124m\"\u001b[39m, secrecy_rate_objective_function(gbest_theta, w_PSO_GD))\n\u001b[0;32m---> 31\u001b[0m gbest_w \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgbest_theta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_PSO_GD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal Best Value: \u001b[39m\u001b[38;5;124m\"\u001b[39m, secrecy_rate_objective_function(gbest_theta, gbest_w))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gbest_theta, gbest_w\n",
      "Cell \u001b[0;32mIn[15], line 90\u001b[0m, in \u001b[0;36mgradient_descent_w\u001b[0;34m(theta, initial_w, learning_rate, total_iter, epsilon)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_iter):\n\u001b[1;32m     89\u001b[0m     current_w \u001b[38;5;241m=\u001b[39m gradient_descent_update_w(theta, current_w, learning_rate)\n\u001b[0;32m---> 90\u001b[0m     current_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43msecrecy_rate_objective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# print(\"Current Value:\", current_value, \"Previous Value:\", previous_value)\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Check for convergence\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(current_value \u001b[38;5;241m-\u001b[39m previous_value) \u001b[38;5;241m<\u001b[39m epsilon:\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36msecrecy_rate_objective_function\u001b[0;34m(theta, w)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Legitimate user k\u001b[39;00m\n\u001b[1;32m      6\u001b[0m numGamma_bk \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mdot(hib[k] \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(theta[k]) \u001b[38;5;241m@\u001b[39m Hai \u001b[38;5;241m+\u001b[39m hab[k], w[k]))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 7\u001b[0m denGamma_bk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhib\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mHai\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_users\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      8\u001b[0m gamma_bk \u001b[38;5;241m=\u001b[39m numGamma_bk\u001b[38;5;241m/\u001b[39mdenGamma_bk\n\u001b[1;32m      9\u001b[0m C_bk \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m gamma_bk)\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Legitimate user k\u001b[39;00m\n\u001b[1;32m      6\u001b[0m numGamma_bk \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mdot(hib[k] \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(theta[k]) \u001b[38;5;241m@\u001b[39m Hai \u001b[38;5;241m+\u001b[39m hab[k], w[k]))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 7\u001b[0m denGamma_bk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msum([np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mdot(hib[k] \u001b[38;5;241m@\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m Hai \u001b[38;5;241m+\u001b[39m hab[k], w[i]))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_users) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m k])\n\u001b[1;32m      8\u001b[0m gamma_bk \u001b[38;5;241m=\u001b[39m numGamma_bk\u001b[38;5;241m/\u001b[39mdenGamma_bk\n\u001b[1;32m      9\u001b[0m C_bk \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m gamma_bk)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "theta_PSO = theta_init.copy()\n",
    "w_PSO = w_init.copy()\n",
    "\n",
    "print(\"Initial Secrecy Rate PSO-GD:\", secrecy_rate_objective_function(theta_init, w_init))\n",
    "\n",
    "PSO_GD_results = []\n",
    "PSO_GD_results.append(secrecy_rate_objective_function(theta_init, w_init))\n",
    "\n",
    "# PSO parameters\n",
    "total_iter = 100\n",
    "num_cycles = 500\n",
    "for i in range(num_cycles):\n",
    "    theta_opt, w_opt = PSO_GD(w_PSO)\n",
    "    theta_PSO, w_PSO = theta_opt, w_opt\n",
    "    print(\"Cycle:\", i+1, \"Objective Function Value:\", secrecy_rate_objective_function(theta_opt, w_opt))\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        PSO_GD_results.append(secrecy_rate_objective_function(theta_opt, w_opt))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABs1ElEQVR4nO3dd3gU1f4G8He2pieEdAghEFroRSCg1NBFKT9BwEu10QQRFVAUUMhFwStwFbgqAVFEUZr0HqTX0In0oCRU05PNlvn9sdklSwJkk53dbPJ+nidPdmdmZ757ErIv55yZEURRFEFERETkhGSOLoCIiIiouBhkiIiIyGkxyBAREZHTYpAhIiIip8UgQ0RERE6LQYaIiIicFoMMEREROS2FowuQmsFgwK1bt+Dp6QlBEBxdDhERERWBKIpIT09HSEgIZLLH97uU+SBz69YthIaGOroMIiIiKoabN2+icuXKj11f5oOMp6cnAGNDeHl52Wy/Wq0W27ZtQ+fOnaFUKm22XyqIbW0fbGf7YDvbB9vZPqRs57S0NISGhpo/xx+nzAcZ03CSl5eXzYOMm5sbvLy8+I9EYmxr+2A72wfb2T7YzvZhj3Z+2rQQTvYlIiIip8UgQ0RERE6LQYaIiIicFoMMEREROS0GGSIiInJaDDJERETktBhkiIiIyGkxyBAREZHTYpAhIiIip8UgQ0RERE6LQYaIiIicFoMMEREROa0yf9NIInsSRdH0wPJ7Ycvyvufb4rHbPOWghT62fGkh+3lk3/rcXMiys6FPS4PscTd/s7aex25ShP0U9/hFeY2N9lPgfRRhtzptLhSpqdDdvg2U9CZ7xW3H0n6sonhKPVqdDop/UqBNSgIU/KgrlA1+psZ2/geGzEzAx6fkNRWDIBb7L4pzSEtLg7e3N1JTU21+9+tNmzahe/fuJbrjpyEnB5orVyDm5EDMzYWYmwtD3ncxV2teJubmQtTpIOq0ELVaQKeDqNXlLdMZl+l1EPUGwKCHaBABvR6iwQAYDBANesAgAgYDIBqM6w0GQBQhigbjOtG4TBTzHpueQzT+gc7b3vwaEQ+3My8XCywD8HAf+T+gTa95wjrTPrVaLZQKBWC6C+rjPvAfDQf5anhSuBAfeV6kx2X7nw4RUZH5f/QR/AYOsOk+i/r5zZhqR6IoQnfrFrLi45EdfwrZ8fHIuXAB0OkcXVqpJwdgcHQRVLaZQrK12zy6rAj7EQ0GCLIijOwXpaYisM1einowux7tifQGA+R2bOfySq/XQ5A7bqYKg4yERFFEzunTyDp+Atnx8cg+eRK6u3cLbCevUAFyLy8IKtVjvpQQlHlfCiUEhcL4pVQAStNzJQSFHJDLjX8gZXLjL5YgA+QyCDI5IJNBkAmATA7IBAiCAMjythGQ9zoZjE8ACIJxmSDkLROMrxcEi2XGb6Z9CebXC0K+bU3LAcvXWKyH5WvzttXp9Yjbuxdt27SFIn/vl3mTfPs17z//MiHfw3x/sJ76OP+yJxzr0dc98kdReNI2hb5GsFwkxQdsvsemR1qdDpu3bEG3rl2f3MtY3Hok2kYoyn5KEVv15tKTsZ3tw9TONbt3d1gNDDIS+ueHH3F75kzLhQoFXGrXhmujRnBt1AhujRtBERLidH+M7Umm1UJ78SJU1cL5B0lCgiAYg3BeaCYicgYMMhLSXLoEAFDXrg2vHt3h1qgRXOrVg8zV1cGVERERlQ0MMhISc3MBAN7P90DFV191cDVERERlj0OvIxMTE4NnnnkGnp6eCAgIQK9evZCQkGCxTbt27SAIgsXXm2++6aCKrWMKMoJK5eBKiIiIyiaHBpm4uDiMHj0ahw4dwvbt26HVatG5c2dkZmZabPfaa68hKSnJ/PXZZ585qGLriFotAAYZIiIiqTh0aGnLli0Wz5cuXYqAgAAcP34cbdq0MS93c3NDUFCQvcsrMXOPDCdOEhERSaJUzZFJTU0FAPj6+los//HHH/HDDz8gKCgIPXv2xNSpU+Hm5lboPjQaDTQajfl5WloaAOMpYtq8HhJbMO3rSfvU59VhkMlteuzypihtTSXHdrYPtrN9sJ3tQ8p2Luo+S82VfQ0GA1544QWkpKRg37595uX/+9//EBYWhpCQEJw+fRrvv/8+mjdvjtWrVxe6n2nTpmH69OkFlq9YseKx4UcqlRctgtu167g1aBAyGtS367GJiIicWVZWFgYOHPjUK/uWmiAzcuRIbN68Gfv27UPlypUfu92uXbvQsWNHXL58GdWrVy+wvrAemdDQUNy7d8/mtyjYvn07OnXq9Nhrm9wcOBCaM2cRvGA+3Nu1s9mxy5uitDWVHNvZPtjO9sF2tg8p2zktLQ1+fn7OcYuCMWPGYMOGDdi7d+8TQwwAtGjRAgAeG2TUajXUanWB5UqlUpJf5ifuV2u89YDC1Y3/kGxAqp8hWWI72wfb2T7YzvYhRTsXdX8ODTKiKGLs2LFYs2YN9uzZg/Dw8Ke+Jj4+HgAQHBwscXUlZ9AYJ/se+SsdmjNJDq7Geen0esTfFyA7dxsKudzR5ZRZbGf7YDvbB9vZPkztXO9BFqoHejukBocGmdGjR2PFihVYt24dPD09kZycDADw9vaGq6srrly5ghUrVqB79+6oWLEiTp8+jbfffhtt2rRBgwYNHFl6kaSlZcIVwGe7ruJivN7R5Tg5OWL/POXoIsoBtrN9sJ3tg+1sH3JUq/2gfAaZhQsXAjBe9C6/2NhYDB06FCqVCjt27MCXX36JzMxMhIaGom/fvvjwww8dUK31RK2xR8bH2x3PVK3g4GqclyiKePDgH/j6VuA9qSTEdrYPtrN9sJ3tw9TO/h6Ou16aw4eWniQ0NBRxcXF2qsb2ZDrjqWPdm1TB4AGtHFyN83p4F9vmHOuWENvZPtjO9sF2tg9TO3esE+CwGhx6Zd+yTqbLm+zrUnDyMREREZUcg4yE5Hk9MgoX3qKAiIhICgwyEhENBsgNxgm+ikJOByciIqKSY5CRiJjv0spKDi0RERFJgkFGIhZBxpVBhoiISAoMMhIx3fkaAFRqzpEhIiKSAoOMRExBRivIoVbx1D8iIiIpMMhIxBxk5AqoFGxmIiIiKfATViLmICOTQyVnMxMREUmBn7ASeRhk2CNDREQkFX7CSoRBhoiISHr8hJWIIX+Q4dASERGRJPgJK5H8PTJqJZuZiIhICvyElYguRwMA0MrlUMvlDq6GiIiobGKQkYjWFGQ4R4aIiEgy/ISVSG42gwwREZHU+AkrEdPQkk6ugFwmOLgaIiKisolBRiI6jTHI6OW8PQEREZFUGGQkYuqRMSgUDq6EiIio7GKQkYgu7/Rrg5xBhoiISCoMMhLR5+QFGQWHloiIiKTCICMRfd4cGVHJIENERCQVBhmJmG5RwB4ZIiIi6TDISMSgMQYZsEeGiIhIMgwyEjH1yECpcmwhREREZRiDjERMN40UGWSIiIgkwyAjETGXQ0tERERSY5CRiCnICOyRISIikgyDjEREbV6PjIo9MkRERFJhkJGKVgsAEFTskSEiIpIKg4xEhLwgI1MzyBAREUmFQUYqeUNLMvbIEBERSYZBRiLskSEiIpIeg4xEBJ0xyMhVagdXQkREVHYxyEhEptMBAORqnrVEREQkFQYZichMPTJq9sgQERFJhUFGIqYgo2CQISIikgyDjETkeuPQksKFQYaIiEgqDDISkbNHhoiISHIMMhIQDQbIDXoAgMKVQYaIiEgqDDISEPOuIQMASg4tERERSYZBRgKmO18DgJI9MkRERJJhkJFA/iCj5hwZIiIiyTDISMAUZLQyOdQquYOrISIiKrsYZCTwMMgooJIzyBAREUmFQUYCFkFGwSYmIiKSCj9lJWBgkCEiIrILfspKgD0yRERE9sFPWQmIucbryGhlcqjkbGIiIiKp8FNWArk5OQCMPTJqJZuYiIhIKvyUlYA2R2P8LmePDBERkZT4KSsBbXZekJEpGGSIiIgkxE9ZCZh6ZPRyBWQywcHVEBERlV0MMhLQmYOM0sGVEBERlW0MMhLQafKCjELh4EqIiIjKNgYZCeg1xuvIGNgjQ0REJCkGGQmYhpYMSgYZIiIiKTHISMDcI6NgkCEiIpISg4wE9HlzZBhkiIiIpMUgIwHTTSNFDi0RERFJyqFBJiYmBs888ww8PT0REBCAXr16ISEhwWKbnJwcjB49GhUrVoSHhwf69u2L27dvO6jiojHdNBIMMkRERJJyaJCJi4vD6NGjcejQIWzfvh1arRadO3dGZmameZu3334bv//+O1atWoW4uDjcunULffr0cWDVT2fQmIKMyrGFEBERlXEOvdDJli1bLJ4vXboUAQEBOH78ONq0aYPU1FR89913WLFiBTp06AAAiI2NRZ06dXDo0CG0bNnSEWU/FXtkiIiI7KNUXbEtNTUVAODr6wsAOH78OLRaLaKjo83b1K5dG1WqVMHBgwcLDTIajQaavMm2AJCWlgYA0Gq10Gq1NqvVtK/C9mnQ5gUZhdKmxyyvntTWZDtsZ/tgO9sH29k+pGznou6z1AQZg8GA8ePHo3Xr1qhXrx4AIDk5GSqVCj4+PhbbBgYGIjk5udD9xMTEYPr06QWWb9u2DW5ubjave/v27QWWKe7fhx+AlKxMbNq0yebHLK8Ka2uyPbazfbCd7YPtbB9StHNWVlaRtis1QWb06NE4e/Ys9u3bV6L9TJ48GRMmTDA/T0tLQ2hoKDp37gwvL6+Slmmm1Wqxfft2dOrUCcpHhpAOfr8KAFDBPwDdu3e32THLqye1NdkO29k+2M72wXa2Dynb2TSi8jSlIsiMGTMGGzZswN69e1G5cmXz8qCgIOTm5iIlJcWiV+b27dsICgoqdF9qtRpqtbrAcqVSKckvc2H7FfK6w+Quav4DsiGpfoZkie1sH2xn+2A724cU7VzU/Tn0rCVRFDFmzBisWbMGu3btQnh4uMX6pk2bQqlUYufOneZlCQkJSExMRFRUlL3LLTJTkJGpeNYSERGRlBzaIzN69GisWLEC69atg6enp3nei7e3N1xdXeHt7Y0RI0ZgwoQJ8PX1hZeXF8aOHYuoqKhSe8YSAAi6vCCjZpAhIiKSkkODzMKFCwEA7dq1s1geGxuLoUOHAgD+85//QCaToW/fvtBoNOjSpQu+/vprO1dqnYc9MgWHuIiIiMh2HBpkRFF86jYuLi746quv8NVXX9mhItsw9cjI2SNDREQkKd5rSQIyBhkiIiK7YJCRgDnIuHBoiYiISEoMMhKQ5wUZZSGngRMREZHtMMhIQK7XAQAU7JEhIiKSFIOMBBhkiIiI7INBxsZEgwFygx4AoGSQISIikhSDjI2JubnmxwwyRERE0mKQsTEx323HGWSIiIikxSBjY/l7ZNSuDDJERERSsurKvikpKVizZg3++OMP3LhxA1lZWfD390fjxo3RpUsXtGrVSqo6nYYpyGhlcqgUcgdXQ0REVLYVqUfm1q1bePXVVxEcHIxPP/0U2dnZaNSoETp27IjKlStj9+7d6NSpEyIjI/Hzzz9LXXOp9jDIKKBWsMOLiIhISkXqkWncuDGGDBmC48ePIzIystBtsrOzsXbtWnz55Ze4efMmJk6caNNCnUX+IKNikCEiIpJUkYLM+fPnUbFixSdu4+rqigEDBmDAgAG4f/++TYpzRgYGGSIiIrsp0ift00JMSbcvSwyafEFGziBDREQkJas/aeVyOdq3b48HDx5YLL99+zbkck5uzc3JAQBo5XKolWwPIiIiKVkdZERRhEajQbNmzXDu3LkC68q73GwNAPbIEBER2YPVn7SCIOC3335Dz549ERUVhXXr1lmsK++0+YKMUs72ICIiklKxemTkcjnmzZuHOXPmoH///vj000/ZG5NHmze0pJMrGOyIiIgkZtUF8R71+uuvo0aNGnjppZewd+9eW9Xk1HQ5xh4ZvVzp4EqIiIjKPqt7ZMLCwiwm9bZv3x6HDh3CzZs3bVqYs9LlnbWkV5QoIxIREVERWP1pe+3atQLLIiIicPLkSdy+fdsmRTkzfY4GcgCinEGGiIhIajY7rcbFxQVhYWG22p3T0uVdEM+g4NASERGR1IrcbVChQoUiTV599Poy5Y1ewyBDRERkL0UOMl9++aX5sSiKGDlyJGbMmIGAgAAp6nJaeo1xsq9BySBDREQktSIHmSFDhlg8Hzt2LPr27Ytq1arZvChnZrpFgcgeGSIiIsnx0rM2ZrpppMgeGSIiIskxyNiYKchAqXJsIUREROUAg4yNiRpTkGGPDBERkdSKPEdmwoQJFs9zc3Mxc+ZMeHt7Wyz/4osvbFOZkxK1eUFGxR4ZIiIiqRU5yJw8edLieatWrXD16lWLZby3ECDmDS0JHFoiIiKSXJGDzO7du6Wso+zQagEAgopDS0RERFIr8hyZNm3aYO7cubh06ZKU9Ti/vKElgUNLREREkitykBkxYgQOHDiAJk2aoE6dOnj//fexf/9+iKIoZX1OR8jrkZExyBAREUmuyEFmyJAh+O2333Dv3j3MnTsXKSkpeOmllxAUFIThw4dj7dq1yM7OlrJW52AKMmoGGSIiIqlZffq1Wq1G9+7dsXjxYty6dQvr169HcHAwpk6diooVK+L555/H/v37pajVKTzskVE7uBIiIqKyr8TXkWnRogVmzpyJM2fO4MyZM+jYsSOSkpJsUZtTkumMQUbOoSUiIiLJFfmsJZObN29CEARUrlwZAHDkyBGsWLECkZGReP311/H222/bvEhnIpiCjAuDDBERkdSs7pEZOHCg+VTs5ORkREdH48iRI/jggw8wY8YMmxfobMw9MmoOLREREUnN6iBz9uxZNG/eHADwyy+/oH79+jhw4AB+/PFHLF261Nb1OR25XgcAULgwyBAREUnN6iCj1Wqhzutt2LFjB1544QUAQO3atcv13BgTU4+MkkNLREREkrM6yNStWxeLFi3CH3/8ge3bt6Nr164AgFu3bqFixYo2L9DZmHtkOLREREQkOauDzOzZs7F48WK0a9cOAwYMQMOGDQEA69evNw85lWcKDi0RERHZjdVnLbVr1w737t1DWloaKlSoYF7++uuvw83NzabFORvRYIDcoAcAKBlkiIiIJGd1kAEAuVwOrVaLP/74AwBQq1YtVK1a1ZZ1OSXTna8BQOni4sBKiIiIygerh5bS09Pxr3/9C5UqVULbtm3Rtm1bVKpUCa+88gpSU1OlqNFp5A8yajcGGSIiIqlZHWReffVVHD58GBs2bEBKSgpSUlKwYcMGHDt2DG+88YYUNTqN/EFGxXstERERSc7qoaUNGzZg69atePbZZ83LunTpgm+++cZ8BlN5ZQoyuTIF3JVyB1dDRERU9lndI1OxYkV4e3sXWO7t7W0x+bc8MgUZrUwBtaLEt7EiIiKip7D60/bDDz/EhAkTkJycbF6WnJyMd999F1OnTrVpcc7GYA4ycqgYZIiIiCRn9dDSwoULcfnyZVSpUgVVqlQBACQmJkKtVuPu3btYvHixedsTJ07YrlInoMvRGL/LFFDJGWSIiIikZnWQ6dWrlwRllA25eUFGK1NArWSQISIikprVQebjjz+Woo4yQWsKMnI5e2SIiIjsoFgXxAOAY8eO4cKFCwCAyMhING3a1GZFOStt9sMeGQWDDBERkeSsDjJ//fUXBgwYgP3798PHxwcAkJKSglatWmHlypWoXLmyrWt0GqYeGZ282PmQiIiIrFCsC+JptVpcuHABDx48wIMHD3DhwgUYDAa8+uqrUtToNExBRi9XOrgSIiKi8sHqroO4uDgcOHAAtWrVMi+rVasWFixYgOeee86mxTkbXY4GAgC9gkGGiIjIHqzukQkNDYVWqy2wXK/XIyQkxCZFOSudxtgjY+DQEhERkV1YHWQ+//xzjB07FseOHTMvO3bsGMaNG4c5c+bYtDhno9cYL4hnYI8MERGRXRSp66BChQoQBMH8PDMzEy1atIBCYXy5TqeDQqHA8OHDy/V1ZvR5c2QYZIiIiOyjSEHmyy+/lLiMskGfmxdklAwyRERE9lCkIDNkyBCp6ygTTENLIntkiIiI7KJIc2QyMzOt2mlRt9+7dy969uyJkJAQCIKAtWvXWqwfOnQoBEGw+OratatVtdiTwRRk2CNDRERkF0UKMhEREfj3v/+NpKSkx24jiiK2b9+Obt26Yf78+UU6eGZmJho2bIivvvrqsdt07doVSUlJ5q+ffvqpSPt2BNPdr8EgQ0REZBdFGlras2cPpkyZgmnTpqFhw4Zo1qwZQkJC4OLign/++Qfnz5/HwYMHoVAoMHnyZLzxxhtFOni3bt3QrVu3J26jVqsRFBRUpP0BgEajgSbvNGgASEtLAwBotdpCTxsvLtO+8u/TkHdcUaG06bHKu8LammyP7WwfbGf7YDvbh5TtXNR9CqIoikXdaWJiIlatWoU//vgDN27cQHZ2Nvz8/NC4cWN06dIF3bp1g1wuL1bBgiBgzZo1Fmc9DR06FGvXroVKpUKFChXQoUMHfPrpp6hYseJj9zNt2jRMnz69wPIVK1bAzc2tWLUVlX75b6hz9ii2N+mCsP7tJT0WERFRWZaVlYWBAwciNTUVXl5ej93OqiAjpcKCzMqVK+Hm5obw8HBcuXIFU6ZMgYeHBw4ePPjYwFRYj0xoaCju3bv3xIawllarxfbt29GpUyco84aS/nh1HIIP78ahTgPwyheTbXas8q6wtibbYzvbB9vZPtjO9iFlO6elpcHPz++pQaZUX4L25ZdfNj+uX78+GjRogOrVq2PPnj3o2LFjoa9Rq9VQq9UFliuVSkl+mS32q9UBAGRqNf/hSECqnyFZYjvbB9vZPtjO9iFFOxd1f1Zf2deRqlWrBj8/P1y+fNnRpRRK0Bon+8pUKgdXQkREVD44VZD566+/cP/+fQQHBzu6lMIxyBAREdmVQ4eWMjIyLHpXrl27hvj4ePj6+sLX1xfTp09H3759ERQUhCtXruC9995DREQEunTp4sCqH0/Im2EtqBlkiIiI7MGhQebYsWNo3/7h2T0TJkwAYLyS8MKFC3H69GksW7YMKSkpCAkJQefOnfHJJ58UOgemNBB0xiDDHhkiIiL7KFaQ+eOPP7B48WJcuXIFv/76KypVqoTly5cjPDwczz77bJH3065dOzzppKmtW7cWpzyHMfXIyEtp0CIiIiprrJ4j89tvv6FLly5wdXXFyZMnzac6p6amYtasWTYv0JmYe2Q4tERERGQXVgeZTz/9FIsWLcI333xjcWpU69atceLECZsW52xkOvbIEBER2ZPVQSYhIQFt2rQpsNzb2xspKSm2qMlpyXXG68goXRhkiIiI7MHqIBMUFFTodVz27duHatWq2aQoZyXTs0eGiIjInqwOMq+99hrGjRuHw4cPQxAE3Lp1Cz/++CMmTpyIkSNHSlGj05DnDS0p2CNDRERkF1aftTRp0iQYDAZ07NgRWVlZaNOmDdRqNSZOnIixY8dKUaPTUOiNQ0sqBhkiIiK7sDrICIKADz74AO+++y4uX76MjIwMREZGwsPDQ4r6nIo8L8goeNYSERGRXVg9tDR8+HCkp6dDpVIhMjISzZs3h4eHBzIzMzF8+HApanQKol4PuWgAAChd2SNDRERkD1YHmWXLliE7O7vA8uzsbHz//fc2KcoZiXkXwwMANYMMERGRXRR5aCktLQ2iKEIURaSnp8PFxcW8Tq/XY9OmTQgICJCkSGcg5uaaHytdXZ6wJREREdlKkYOMj48PBEGAIAioWbNmgfWCIGD69Ok2Lc6Z5A8yKs6RISIisosiB5ndu3dDFEV06NABv/32G3x9fc3rVCoVwsLCEBISIkmRzsAUZHJlCngq5Q6uhoiIqHwocpBp27YtAODatWsIDQ2FTGb19JoyzZB3zymtTAGVgm1DRERkD1affh0WFgYAyMrKQmJiInLzDakAQIMGDWxTmZPJzTG2g1Ymh1rOHhkiIiJ7sDrI3L17F8OGDcPmzZsLXa/X60tclDPS5uQYv8sVUCvZI0NERGQPVn/ijh8/HikpKTh8+DBcXV2xZcsWLFu2DDVq1MD69eulqNEp5GbnG1qSM8gQERHZg9U9Mrt27cK6devQrFkzyGQyhIWFoVOnTvDy8kJMTAx69OghRZ2lnjYvyOhkCshkgoOrISIiKh+s7jrIzMw0Xy+mQoUKuHv3LgCgfv36OHHihG2rcyLanLwgI7c6GxIREVExWR1katWqhYSEBABAw4YNsXjxYvz9999YtGgRgoODbV6gszDNkdHJlQ6uhIiIqPywuvtg3LhxSEpKAgB8/PHH6Nq1K3788UeoVCosXbrU1vU5DV2OBjIAegV7ZIiIiOzF6k/dV155xfy4adOmuHHjBi5evIgqVarAz8/PpsU5E51GAxUAg4I9MkRERPZi1dCSVqtF9erVceHCBfMyNzc3NGnSpFyHGADQ5V1HxsA5MkRERHZjVZBRKpXIyZsLQpb0eVf2ZY8MERGR/Vg92Xf06NGYPXs2dDqdFPU4LX2uFgCDDBERkT1ZPQ5y9OhR7Ny5E9u2bUP9+vXh7u5usX716tU2K86ZmHpkRAYZIiIiu7E6yPj4+KBv375S1OLUDBrjHBlRySBDRERkL1YHmdjYWCnqcHpirrFHBgwyREREdsObAtmIIW+OjKhUObgSIiKi8oNBxkbEXOPQEntkiIiI7IdBxkZMQUZgjwwREZHdMMjYiLlHRsUeGSIiInuxOshcvXpVijqcn9Y4R0ZQsUeGiIjIXqwOMhEREWjfvj1++OEHXuU3P62xR0amUju4ECIiovLD6iBz4sQJNGjQABMmTEBQUBDeeOMNHDlyRIranIqQ1yMjU7NHhoiIyF6sDjKNGjXCvHnzcOvWLSxZsgRJSUl49tlnUa9ePXzxxRe4e/euFHWWeuYgw6ElIiIiuyn2rZoVCgX69OmDHj164Ouvv8bkyZMxceJETJkyBf369cPs2bMRHBxsy1pLNUFnDDJyTvYlInoig2jAg5wHuJ15G8mZyUjOSrZ4nJ6bXuJjiKKI9PR0xG6MhSAINqiaCmNqZ2WiEt2rd3dIDcUOMseOHcOSJUuwcuVKuLu7Y+LEiRgxYgT++usvTJ8+HS+++GK5GnIyBRmZmnNkqPzK0mZh8h+T8VfGX44updQSRRHpaelYtmlZufyAzdRm4nbWbegM9rnx8J3UO3Y5TnmXlpvmsGNbHWS++OILxMbGIiEhAd27d8f333+P7t27QyYzjlKFh4dj6dKlqFq1qq1rLdVkeXcDl3OODJVjS84uwa6buxxdhlNITkl2dAkOJRNk8HP1Q5BbEALdAxHkHoRAN+N3H7UPBJQs5On0Ohw+dBgtWraAQl7s/7PTU5jauU1IG4fVYPVPd+HChRg+fDiGDh362KGjgIAAfPfddyUuzpnI8npkFC7skaHyKTkzGcvOLQMATGw2ETUr1HRwRaWTXq/H4cOH0aJFC8jlckeXY3euClcEugXCz80PSpl0Q/FarRZ3lXfxTOAzUPKK65IxtbO/m7/DarA6yFy6dOmp26hUKgwZMqRYBTkruZ5Bhsq3L098iRx9DpoGNsXgyMHlctikKLRaLe4r76NFUAt+wBLZgNVnLcXGxmLVqlUFlq9atQrLli2zSVHOyDS0pODQEpVDZ+6ewcarGyFAwLvPvMsQQ0R2Y3WQiYmJgZ+fX4HlAQEBmDVrlk2KckYK9shQOSWKIj47+hkAoGf1nqhbsa6DKyKi8sTqIJOYmIjw8PACy8PCwpCYmGiTopyRXG/skVEyyFA5s/X6VsTfjYerwhXjmoxzdDlEVM5YHWQCAgJw+vTpAstPnTqFihUr2qQoZ6TICzIqVwYZKj80eg3+c/w/AIBh9YYhwC3AwRURUXlj9WTfAQMG4K233oKnpyfatDGebhUXF4dx48bh5ZdftnmBzkDU6yEXDQAAlYurg6shsp/l55fjVuYtBLgFYEikE0/wT0kE7l+2y6EEnR7+aWchXHUDFKXkrCWdBtCkAzmpxu+atLznaQ+f6zSOrtIqclHEcykpkN+eB3DOlmRM7SxUB1DvRYfUYHWQ+eSTT3D9+nV07NgRCoXx5QaDAYMHDy63c2TE3FzzYyV7ZKicuJd9D9+c/gYAML7JeLgp3RxcUTFl/wN83QqwwdVki0IBoBUAXLHL4cotGQBfAMhycCFlnKmddZmOuz2R1UFGpVLh559/xieffIJTp07B1dUV9evXR1hYmBT1OYX8QUbl6uLASojs578n/4ssXRbqVqyLHtV6OLqc4ks8ZAwxSnfAt+D8P1sTRRFpaWnw8vIqPWd3yZWA2gtQewIu3vkeez18rHBxqp4NnU6HY8ePo1nTpub/dJPtmdq5afUODquh2D/dqlWrQhRFVK9evdz/kpiCjAECVGpeF4LKvoQHCVh9aTUA4L1n3oNMsHq6XemReMj4vX5f4IUFkh9Op9Viz6ZN6N69O68jIyFRq8XtywaINbsCbGfJmNoZ3qEOq8Hqvz5ZWVkYMWIE3NzcULduXfOZSmPHjsW///1vmxfoDPQ5xrFjrUwOtbKUjHkTSUQURXx+7HOIENE5rDOaBDZxdEklYwoyoS0dWwcRFYvVQWby5Mk4deoU9uzZAxeXh8Mo0dHR+Pnnn21anLPIzQsyOpkCKoUT/8+UqAji/orD4aTDUMqUeLvp244up2R0GuDWSePjKgwyRM7I6jGhtWvX4ueff0bLli0txnfr1q2LK1fK5+y13JwcAHk9MgwyVIZp9VrMPTYXAPCvyH+hsmdlB1dUQrfiAb0GcPcHfKs5uhoiKgarP3Xv3r2LgICC14rIzMwsPRPX7Cw3O29oSa6ASs4gQ2XXzwk/43radfi6+OK1+q85upySSzxo/B7awqkmshLRQ1b3yDRr1gwbN27E2LFjAcAcXr799ltERUXZtjonoc0xTvbVyRTlNswVx6m7pzDz0Ew8yHnw1G2zs7OxYK30EzHLu6e1s+lnNabxGHioPOxVlnRuHjZ+57ASkdOyOsjMmjUL3bp1w/nz56HT6TBv3jycP38eBw4cQFxcnBQ1lnpa0xwZOWfGF1ViWiLG7ByDFE1KkV+TlpUmXUFk9rR2jqwYiT4RfexUjYRE8eFE3yrl8z9hRGWB1UHm2WefxalTpxATE4P69etj27ZtaNKkCQ4ePIj69etLUWOpp802BZnyfRp6UaXkpGDUzlFI0aSgbsW6+LDlh0/sydLpdNi/bz9aP9u63J/qL6WitLMAAeHe4ZDLysDZefcuAdkPjNdHCWrg6GqIqJis+lTQarV44403MHXqVHzzzTdS1eR0dDkayAHoGWSeKlefi3G7x+FG2g2EuIfgvx3/Cz/XgndTz0+r1eK64joifSN53Q0Jlbt2vpnXG1OpKaBQObYWIio2q2amKpVK/Pbbb1LV4rR0GuNZSwZFOfjjXwKiKGLq/qk4cecEPJQe+KrjV08NMUSSSeT8GKKywOpTbHr16oW1a9dKUIrz0mmMQ0t6Bpkn+vrU19h0bRMUggJftPsCERUiHF0SlWfmM5YYZIicmdVjITVq1MCMGTOwf/9+NG3aFO7u7hbr33rrLZsV5yz0eWctGTh/47HWXV6HRacWAQCmRk1FVAgnV5IDZdwFHuRd9yr0GcfWQkQlYvUn73fffQcfHx8cP34cx48ft1gnCEL5DDJ5PTIcWirckaQjmHZwGgDg1fqvok+NMnDGCzk30/yYgEjAtYJjayGiErF6aOnatWuP/bp69apV+9q7dy969uyJkJAQCIJQYMhKFEV89NFHCA4OhqurK6Kjo3Hp0iVrS5acXmPskREZZAq4mnIV4/eMh86gQ9eqXTG28VhHl0SU7/5KLRxbBxGVmEMvQ5uZmYmGDRviq6++KnT9Z599hvnz52PRokU4fPgw3N3d0aVLF+Tk3RKgtDAFGUN5ONPDCvez72PUzlFIz01HI/9G+PTZT537LslUdvBCeERlhtVDS3379kXz5s3x/vvvWyz/7LPPcPToUaxatarI++rWrRu6detW6DpRFPHll1/iww8/xIsvvggA+P777xEYGIi1a9fi5ZdfLvR1Go0GmryhHgBISzNe3Eur1UKr1Ra5tqcx7Uur1UKfd9YSFEqbHsOZJWcm4/197+PvjL9R2aMy5j43FzKDDFqD9e2Tv61JOuWmnbXZUNyKhwBAG9IMsPP7LTft7GBsZ/uQsp2Luk9BFEXRmh37+/tj165dBS5+d+bMGURHR+P27dvW7O5hIYKANWvWoFevXgCAq1evonr16jh58iQaNWpk3q5t27Zo1KgR5s2bV+h+pk2bhunTpxdYvmLFCri5uRWrtqfJWrUFjY7twb66zyJg8POSHMMZpBhScC73HM5qz+Km/iYAwFVwxRseb8BPztOsqXSomHERz16ahRyFD7bWm8d7LBGVUllZWRg4cCBSU1Ph5eX12O2s7pHJyMiASlXw4lFKpdLc+2ELycnJAIDAwECL5YGBgeZ1hZk8eTImTJhgfp6WlobQ0FB07tz5iQ1hLa1Wi+3bt6NTp07Yv814Gqe7tw+6d+9us2M4g1uZt7AzcSe2J27H2ZSz5uUCBDQOaIy3G7+NuhXrlugY+du6XFyozUHKSzvL9v8JXAJUNdqie48edj9+eWlnR2M724eU7VzUTGF1kKlfvz5+/vlnfPTRRxbLV65cicjISGt3Z3NqtRpqtbrAcqVSKckvs1KpNHdNCyq1w/7BGEQDrqdeh0avsVguQizwXKvXQmvI+8r3OFefa378NOm56dhzcw/O3DtjXiZAQNPApuhctTOiq0TD383fJu/NRKqfIVkq8+389xEAgCwsCjIHvs8y386lBNvZPqRo56Luz+ogM3XqVPTp0wdXrlxBhw4dAAA7d+7ETz/9ZNX8mKcJCgoCANy+fRvBwcHm5bdv37YYaioNxFzjZF9BZf9/LJnaTKy7vA4/XfwJ19Ou2/34pvDSpWoXRIdF80q9VLoZDA8n+vKMJaIyweog07NnT6xduxazZs3Cr7/+CldXVzRo0AA7duxA27ZtbVZYeHg4goKCsHPnTnNwSUtLw+HDhzFy5EibHccWRJ2pR8Z+92u5lnoNKy+uxLor65CpzQQAuMhd4KV++vCZUqaESq6CUqYs8FgpN35/GoWgQJPAJgwv5FzuJQA5qYDSnTeKJCojinUp2h49eqCHDcaWMzIycPnyZfPza9euIT4+Hr6+vqhSpQrGjx+PTz/9FDVq1EB4eDimTp2KkJAQ84TgUkNrnyBjEA34468/sOLiChy4dcC8PNw7HANqD8AL1V+Au9L9CXsgKudMtyWo3BTgTV6JyoRi/UtOSUnBr7/+iqtXr2LixInw9fXFiRMnEBgYiEqVKhV5P8eOHUP79u3Nz02TdIcMGYKlS5fivffeQ2ZmJl5//XWkpKTg2WefxZYtW+Di4lKcsiUjaI1DSzKJgkyOLge/JPyClQkrcTPdeDaQAAFtKrfBwDoDERUcBYFnXhA9nelGkby/ElGZYXWQOX36NKKjo+Ht7Y3r16/j1Vdfha+vL1avXo3ExER8//33Rd5Xu3bt8KSzvwVBwIwZMzBjxgxry7QrIa9HRqa2fZC5l30Pb+16yzyp1lPpid41euPlWi8j1CvU5scjKtNMtybghfCIygyrg8yECRMwdOhQfPbZZ/D09DQv7969OwYOHGjT4pyFOcjYuEfmaspVjNo5Cn9n/A1vtTfeavwWnq/2PNyU0lwPh6hMS08G/rkOCDKgMm8USVRWWB1kjh49isWLFxdYXqlSpSde36UsE/Im+8pt2CNzNPkoxu0eh/TcdIR6huLrjl+jqndVm+2fqNwx3V8poC7gYrtrShGRY1l94xu1Wl3oRWr+/PNP+Pvb9rohzkJmDjIFr19THL9f+R2vb38d6bnpaOjfED90/4EhhqikEjmsRFQWWR1kXnjhBcyYMcN8DwRBEJCYmIj3338fffv2tXmBzkCm0wEA5C4l65ERRRELTy3ElH1ToDPo0DmsM77t/C18XXxtUSZR+cb5MURlktVBZu7cucjIyEBAQACys7PRtm1bREREwNPTEzNnzpSixlLP1COjUBf/bCqtXosP93+Ir+O/BgAMqzcMn7f9HC6K0nWGFpFTys0Ekk4bH/NCeERlitVzZLy9vbF9+3bs378fp06dQkZGBpo0aYLo6Ggp6nMKcr0pyBRvaCktNw1v734bR5KPQC7IMaXFFPSr1c+WJRKVb38dA0Q94FUZ8OHZfkRlSbGvCNW6dWu0bt3alrU4LXne0JLCxfogczvzNt7Y/gaupF6Bm8INc9rOwXOVn7N1iUTlm+m2BFXYG0NU1hR5aOngwYPYsGGDxbLvv/8e4eHhCAgIwOuvvw6NRvOYV5dtirweGWUx5sjMPjobV1KvIMAtAN93+54hhkgKpom+vBAeUZlT5CAzY8YMnDt3zvz8zJkzGDFiBKKjozFp0iT8/vvviImJkaTI0k6uN/bIKK3skbmeeh07buwAAHzd8WvU8q1l89qIyj2DHrhpvOM1J/oSlT1FDjLx8fHo2LGj+fnKlSvRokULfPPNN5gwYQLmz5+PX375RZIiSzuloXhBZum5pRAhom3ltgwxRFK5cx7ITQdUnkBgXUdXQ0Q2VuQg888//yAwMND8PC4uDt26dTM/f+aZZ3Dz5k3bVucERJ0OsrzbLKjdih5k7mbdxfor6wEAw+sNl6Q2IsLDYaXKzQCZ3LG1EJHNFTnIBAYG4tq1awCA3NxcnDhxAi1bPuymTU9Ph1KptH2FpZyYdz0dAFC5uBb5dcsvLIfWoEUj/0ZoEthEitKICMh3Ibwox9ZBRJIo8llL3bt3x6RJkzB79mysXbsWbm5ueO65hxNTT58+jerVq0tSZGkm5uYLMkXskUnPTceqhFUAgBH1R0hSF5UD2mzg3iXg/mVAl1Pi3Ql6PULvn4JwOg2Ql6Gei+v7jN95xhJRmVTkIPPJJ5+gT58+aNu2LTw8PLBs2TKo8t0kccmSJejcubMkRZZmurwztQwQoCrivZZ+TvgZGdoMVPeujjaV20hZHpUFWQ+AuwnAvQRjcLmbANz7E0hJBPD4u8dbSwGgCQAk2myXpYcgByo1c3QVRCSBIgcZPz8/7N27F6mpqfDw8ID8kf+xrVq1Ch4eHjYvsLTLzTb+T1grk0OtfPr/YjV6DX44/wMAYHj94ZAJVl9cuXy5tB2KjRPRPe02FOcVAARHV2RfBj2gzXz8etcKgF9NQF3ymyAaRBF3796Bv38AZEIZa+daXQF1+fv7RFQeFOvKvoXx9S2f9wPKzTb2yGhlCqjkTw8l66+sx/2c+whyD0K38G5P3b5cO7cW+O1VCAYtlABQPi9TZOQdagws/rUAvxqAXy3jc3c/wEahQ6/V4tCmTejevTtk5XC+GxE5p2Jf2ZeMtDl5QUaugFL+5A8UvUGPpWeXAgCGRA6BUsYPi8eK/wlYNwoQDTBE9sYuMQpt27WDUlEOf2U9gwCVu6OrICIqlcrhp4JtmYKMTqaA8JT/Ge9I3IHE9ER4q73Rp0Yfe5TnnI5+B2ycYHzc+BXou85F5patgG81gD0FRESUDydolJA2b46MTv7kTCiKIr478x0AYGDtgXBTuklem1M6sOBhiGnxJtBzAa/9QUREj8UemRLS5uRCDkAvf3JPwaGkQ7jw4AJc5C4YUHuAfYpzJqIIxM0G9uTd5uLZCUDHj4zzP/R6x9ZGRESlFoNMCWlzNJDj6T0yS84uAQD0qdEHFVwq2KEyJyKKwPaPgAPzjc87TAXaTHRsTURE5BQYZEpIn3cdGf0TJqGeu38Oh5IOQS7IMaTuEHuV5hwMBmDzu8DRb43Pu8QAUaMcWxMRETkNBpkSMl0QT3zC0NKSM8bemG7h3RDiEWKXuhxOrwWuxgGatCdvl7AZOPMLAAHoOQ9oyqBHRERFxyBTQnpNrvH7Y3pkbqTdwI7EHQCAYfWG2a0uh7p/BVj9GvD38aJtL8iB3ouBBi9JWxcREZU5DDIlZAoyBkXhPTJLzy2FQTSgTeU2qFmhpj1Lsz9RBOJ/BDa9Z7wardobCG7w5NfIVUDLkUCNTvapkYiIyhQGmRIyBRmxkCBzL/se1l9eDwAYUa+M3xwy+x/g9/HA+bXG52Gtjb0sPqGOrIqIiMo4BpkS0ufm9cgUcqG2X//8FbmGXDTyb4QmgU3sXZr9XPsDWPMGkPY3IFMA7T8AWo/j9V+IiEhyDDIlJOYFmcKuOHvpn0sAgE5hZXTYRJcL7J4J7J8HQAR8qwN9vwUqleHQRlSK6fV6aLVaR5dRKmi1WigUCuTk5EDPa1FJpiTtrFQqC9yAujgYZErIYBpaUqoKrEvOSgYABHsE27Umu7h3CfjtVSAp3vi8yWDjqdO8wzCR3YmiiOTkZKSkpDi6lFJDFEUEBQXh5s2bT719DBVfSdvZx8cHQUFBJfoZMciUkKh9fI9McmZekHG3U5DRZADpSUB6MpBx2/iVngxk3AEykoH020DWPUA0lPxYOWmAQQu4VgB6zgciXyj5PomoWEwhJiAgAG5ubvzgBmAwGJCRkQEPDw/IZLwbj1SK286iKCIrKwt37twBAAQHF/9zkkGmhMRcYzeu8EiPjNagxb3sewCAIPcg6Qs5+xuwdhSgy5H+WCbhbYwTer3KybVxiEohvV5vDjEVK1Z0dDmlhsFgQG5uLlxcXBhkJFSSdnZ1dQUA3LlzBwEBAcUeZmKQKaHHzZG5m3UXBtEAhUwBXxdfaYvIuANseNsYYlQegGcQ4BEEeATkPQ40fnkGAu7+gMwGd5BWqIAK4cZ7IRGRw5jmxLi58Ua05HxMv7darZZBxmHyhpYElWWPjGlYKdAtEDJB4v8NbPsQyEkFghsCr+4CnnLfJyIqezicRM7IFr+37G8rqbz/DT0uyEg+rHQ1Djj9MwABeP4/DDFERFSuMMiUVF6QkT0aZLLsMNFXpwE2TjA+fuZVoFJT6Y5FRFQGDB06FL169TI/b9euHcaPH++weqjkGGRKSDD1yKgd0COz70vg/mXj/JeOU6U7DhGRBJKTkzFu3DhERETAxcUFgYGBaN26NRYuXIisrCy71LB69Wp88sknNt3no2HpSdsJggBBEKBUKhEYGIhOnTphyZIlMBhscHapHU2fPh2NGjVyyLE5DlFCgs44R0aufEyQcZMoyNy/Avwx1/i4awzg4i3NcYiIJHD16lW0bt0aPj4+mDVrFurXrw+1Wo0zZ87gf//7HypVqoQXXij8sg5arRbKQi55URy+vhKfjPEUXbt2RWxsLPR6PW7fvo0tW7Zg3Lhx+PXXX7F+/XooHnNDYnqIPTIlJNPqjN/t2SMjisDGdwC9BqjeAajbx/bHICKS0KhRo6BQKHDs2DH069cPderUQbVq1fDiiy9i48aN6Nmzp3lbQRCwcOFCvPDCC3B3d8fMmTOh1+sxYsQIhIeHw9XVFbVq1cK8efMsjqHX6/HOO+/Ax8cHFStWxHvvvQdRFC22eXRoSaPRYOLEiahUqRLc3d3RokUL7Nmzx7x+6dKl8PHxwdatW1GnTh14eHiga9euSEpKAgBMmzYNy5Ytw7p168y9Lflf/yi1Wo2goCBUqlQJTZo0wZQpU7Bu3Tps3rwZS5cuNW+XkpKCV199Ff7+/vDy8kKHDh1w6tQp8/pTp06hffv28PT0hJeXF5o2bYpjx46Z1+/fvx/t2rWDm5sbKlSogC5duuCff/4BYDyFOiYmxtyWDRs2xK+//mp+7Z49eyAIAnbu3IlmzZrBzc0NrVq1QkJCAgBgxYoVmDFjBk6dOmV+z/lrlxqDTAkJOuPQktyeQebsb8DV3YBcDXSfw1OgiciCKIrIytXZ/evRkPA49+/fx7Zt2zB69Gi4u7sXus2jZ7NMmzYNvXv3xpkzZzB8+HAYDAZUrlwZq1atwvnz5/HRRx9hypQp+OWXX8yv+e9//4tly5ZhyZIl2LdvHx48eIA1a9Y8sbYxY8bg4MGDWLlyJU6fPo2XXnoJXbt2xaVLl8zbZGVlYc6cOVi+fDn27t2LxMRETJw4EQAwceJE9OvXzxxukpKS0KpVqyK1i0mHDh3QsGFDrF692rzspZdewp07d7B582YcP34cTZo0QceOHfHgwQMAwKBBg1C5cmUcPXoUx48fx6RJk8y9VvHx8ejYsSMiIyNx8OBB7Nu3Dz179jTfUiAmJgbff/89Fi1ahHPnzuHtt9/GK6+8gri4OIu6PvjgA8ydOxfHjh2DQqHA8OHDAQC9e/fGhAkTULduXfN77t+/v1XvuSTYZ1VCsrwgo3BRm5fl6HLwj8aYdG0eZLJTgC2TjY/bTAQqVrft/onI6WVr9Yj8aKvdj3t+Rhe4qZ7+sXL58mWIoohatWpZLPfz80NOjvGinqNHj8bs2bPN6wYOHIhhw4ZZbD99+nTz4/DwcBw8eBC//PIL+vXrBwBYtGgRJk2ahD59+pifb936+HZJTExEbGwsEhMTERJivNDnxIkTsWXLFsTGxmLWrFkAjENbixYtQvXqxr+/Y8aMwYwZMwAAHh4ecHV1hUajQVBQ8f/+165dG6dPnwYA7Nu3D0eOHMGdO3egVhs/a+bMmYO1a9fi119/xeuvv47ExES8++67qF27NgCgRo0a5n199tlnaNasGb7++mvzsrp16wIw9kDNmjULO3bsQFRUFACgWrVq2LdvHxYvXoy2bduaXzNz5kzz80mTJqFHjx7IycmBq6srPDw8oFAoSvSei4tBpoRkeuPQkkL9MMjczroNAHBVuMJL5WXbA+76BMi8A1SsYbzDNBFRGXHkyBEYDAYMGjQIGo3GYl2zZs0KbP/VV19hyZIlSExMRHZ2NnJzc80TTlNTU5GcnIzmzZubt1coFGjWrNlje47OnDkDvV6PmjVrWizXaDQWV012c3MzhxjAeHl906X2bUUURXOv1KlTp5CRkVHgys3Z2dm4cuUKAGDChAl49dVXsXz5ckRHR+Oll14y1xgfH4+XXnqp0ONcvnwZWVlZ6NTJ8ubGubm5aNy4scWyBg0amB+bbilw584d+Pj4FP+N2gCDTAnJTUNLLg+HlvIPK9n0IlV/HQeOfmd8/PwXgEL95O2JqFxyVcpxfkYXhxy3KCIiIiAIgnmOhUm1atWM+8m7dH1+jw5BrVy5EhMnTsTcuXMRFRUFT09PfP755zh8+HAxqwcyMjIgl8tx/PjxAleZ9fB4eEPcRycaC4JQ5GG1orpw4QLCw8PNdQUHBxc618YUIqZNm4aBAwdi48aN2Lx5Mz7++GOsXLkSvXv3LrQ9TTIyMgAAGzduRKVKlSzWqdWWnzH537fps600nF3FIFNCcnOPjIt5WVKmcdKXTc9Y0uuADeMAiECDl433OSIiKoQgCEUa4nGUihUrolOnTvjvf/+LsWPHPnaezJPs378frVq1wqhRo8zLTL0TAODt7Y2goCAcOXIE7dq1AwDodDrz/JLCNG7cGHq9Hnfu3MFzzz1ndU0mKpXKPP+kOHbt2oUzZ87g7bffBgA0adIEycnJUCgUqFq16mNfV7NmTdSsWRNvv/02BgwYgNjYWPTu3RsNGjTAzp07LYbiTCIjI6FWq5GYmGgxjGStkr7nkuBk3xIyBRml68PkKslE3yP/A5LPAC4+QOdPbbdfIiIH+Prrr6HT6dCsWTP8/PPPuHDhAhISEvDDDz/g4sWLT73vTo0aNXDs2DFs3boVf/75J6ZOnYqjR49abPPGG29g9uzZWLt2LS5evIhRo0YhJSXlsfusWbMmBg0ahMGDB2P16tW4du0ajhw5gpiYGGzcuLHI761q1ao4ffo0EhIScO/ePfP9sAqj0WiQnJyMv//+GydOnMCsWbPw4osv4vnnn8fgwYMBANHR0YiKikKvXr2wbds2XL9+HQcOHMAHH3yAY8eOITs7G2PGjMGePXtw48YN7N+/H0ePHkWdOnUAAJMnT8bRo0cxatQonD59GhcvXsTChQtx7949eHp6YuLEiXj77bexbNkyXLlyBSdOnMCCBQuwbNmyIr/nsLAwXLt2DfHx8bh3716BoUEpld7I7iQUeuMvqNJFwiCT+jewe6bxcafpgIe/bfZLROQg1atXx8mTJzFr1ixMnjwZf/31F9RqNSIjIzFx4kSLnpbCvPHGGzh58iT69+8PQRAwYMAAjBo1Cps3bzZvM2bMGPzzzz8YMmQIZDIZhg8fjt69eyM1NfWx+42NjcWnn36Kd955B3///Tf8/PzQsmVLPP/880V+b6+99hr27NmDZs2aISMjA7t37zb3Cj1qy5YtCA4OhkKhQIUKFdCwYUPMnz/fXDNg7GHbtGkTPvjgAwwbNgx3795FUFAQ2rRpg8DAQMjlcty/fx+DBw/G7du34efnhz59+ph7YGrWrIlt27ZhypQpaN68OVxdXdGiRQsMGDAAAPDJJ5/A398fMTExuHr1Knx8fMynghdV3759sXbtWrRv3x4pKSmIjY3F0KFDi/z6khBEWw/slTJpaWnw9vZGamoqvLxsN/FWq9Vi06ZNCJsyFWq9FuKKNYhsYpwt/uaON7H/7/2Y0WoGetfoXbIDiSLw08vAn1uAys2B4VuBcnZLelNbd+/e3WYXwaKC2M72Yet2zsnJwbVr1xAeHg4XF5env6CcMBgMSEtLg5eXlzkQkO2VtJ2f9Ptb1M9v/nRLSGkwDi2pXB/+AG5nGs9aCnQPLPkBTq00hhi5Cug5r9yFGCIioifhp2JJ6PWQ5XVoqaSYI5OWBGx53/i47ftAYGTJ9kdERFTGMMiUgJBvhrY6r0cmPTcdGVrj6WwlOmtJFIHfxwE5qUBIY6D1+JKUSkREVCYxyJSAIe8+S8DDHhlTb4yXygtuSrfi7/zUT8ClrcYhpV4LATnnZRMRET2KQaYEDLnGIKOHAHXeBfFMQSbYPbj4O067BWyeZHzcbhIQUKdEdRIREZVVDDIlYOqR0cnkUMmNTZmcVcL5MaYhJU0qENIEaMXbEBARET0Og0wJGLTGOTJauQIKU5Ap6UTf+BXApW0cUiIiIioCBpkSeNgj8zBslCjIpN16eGfrdpOBgNolrpGIiKgsY5ApAYMuL8jICwaZQDcrryEjisD6t4xDSpWaAq3eslmdREREZRWDTAmIeUNLOvnDq3MWe7Jv/I/A5e3GIaUXv+aQEhERUREwyJSAaWhJnxc6RFHE7SzjVX2tGlpK/fvhkFL7KRxSIqJyYejQoRAEAYIgQKVSISIiAjNmzIAur7f7m2++QcOGDeHh4QEfHx80btwYMTExFvt48OABxo8fj7CwMKhUKoSEhGD48OFITEx0xFsiB+B/+0tA1Bl7ZExB5h/NP9DoNRAgFH1oSRSB398CNGnGIaWosVKVS0RU6nTt2hWxsbHQaDTYtGkTRo8eDaVSicDAQIwfPx7z589H27ZtodFocPr0aZw9e9b82gcPHqBly5ZQqVRYtGgR6tati+vXr+PDDz9EixYtsHXrVjRo0MCB747sgUGmBMxBRmEcWjINK1V0rQilXAlkpwBnVgGZ94xBJSfNOAdGk573OM145d7Mu4BczbOUiMg2RBHQZtn/uEo3QBCseolarUZQkLEHe+TIkVizZg3Wr1+PwMBA9OvXDyNGjDBvW7duXYvXfvDBB7h16xYuX75s3keVKlWwdetW1KhRAxMnTsS2bdtK+KaotOOnZgmIed2fhrwgk5SZBCDfrQn+mAMcWPD0HQlyoMtMwL+WJHUSUTmjzQJmhdj/uFNuASr3Eu3C1dUV9+/fR1BQEOLi4nDjxg2EhYUV2M5gMGDlypUYNGiQOcTk38fIkSMxdepUPHjwAH5+fiWqiUq3Uh1kpk2bhunTp1ssq1WrFi5evOigih6htQwyBU69vnnE+L1GZyAgEnDxAtRegIs3oPbMe+wFuAcAnja4UzYRkZMSRRE7d+7E1q1bMXbsWEyYMAF9+vRB1apVUbNmTURFRaF79+74v//7P8hkMty9excpKSmoU6fwK5/Xrl0boiji8uXLDDJlXKkOMoCxK3HHjh3m5wpFKSrZ3CNjrOl2Zr6JvgY9kHzGuF3nT9nbQkT2o3Qz9o444rhW2rBhAzw8PKDVamEwGDBw4EBMmzYN7u7uOHjwIM6ePYu9e/fiwIEDGDJkCL799lts2bLF/HpRFG35DsgJlaJUUDiFQlGg27DUyJsjIxbWI3P/srF7V+kGVIxwWIlEVA4JQomHeOylffv2WLhwofmMo0f/s1qvXj3Uq1cPo0aNwptvvonnnnsOcXFxaNu2LXx8fHDhwoVC93vx4kUIgoCICP79LetKfZC5dOkSQkJC4OLigqioKMTExKBKlSqP3V6j0UCj0Zifp6WlAQC0Wi20Wq3N6tJqtRDyzZHRarXmOTL+Lv7Q/XUcCgCGwHrQ6w2A3mCzY5c3pp+bLX9+VBDb2T5s3c5arRaiKMJgMMBgcK6/M6Iows3NDdWqVTMve9J7qF3beGmK9PR0AMBLL72EFStWYNq0aRb/4c3OzsbChQvRoUMHVKhQwenaxZmYesRMv4PWMhgMEEURWq0WcrncYl1R/42U6iDTokULLF26FLVq1UJSUhKmT5+O5557DmfPnoWnp2ehr4mJiSkwrwYAtm3bBjc367s9nygvyGRptdi0aROupV4DAFyNv4rryX8gAsD1HC+c2bTJtsctp7Zv3+7oEsoFtrN92KqdTb3WGRkZyM3Ntck+7UWr1UKn05n/w5nfhAkTEBwcjOeeew4hISG4ffs25syZAz8/P9SrVw9paWmYNGkSduzYgejoaEyfPh116tTBjRs3MHPmTOTm5mLOnDnm0EPSKm475+bmIjs7G3v37jVfP8gkK6toZ94JohMNMKakpCAsLAxffPGFxSl5+RXWIxMaGop79+7By8vLZrVotVr8+uZ7eObITpx/Jho9vv0cLX9uCb2ox5ZeWxC8+nXIbuyD7vn5EBsOtNlxyyOtVovt27ejU6dOUCqVT38BFQvb2T5s3c45OTm4efMmqlatChcXFxtUaD/Dhg1DSkoK1qxZU2Ddb7/9htjYWMTHx+P+/fvw8/NDy5Yt8dFHH6F+/frm7e7du4dPPvkE69atQ3JyMnx9fdG1a1d8/PHHqFChAjw9PSFYeUo4FZ0oikhPTy92O+fk5OD69esIDQ0t8PublpYGPz8/pKamPvHzu1T3yDzKx8cHNWvWxOXLlx+7jVqthlqtLrBcqVTa/I+zkDdHRlCpkKJLgV7UQyEoEOQRCFneRF9F5SYAPxRsQoqfIRXEdrYPW7WzXq+HIAiQyWSQyZzrYu3Lli177LqXXnoJL7300lP3ERAQgAULFmDBAstLXRgMBqSlpZnbhqRhGk4qbjvLZDIIglDov4ei/vtwqp9uRkYGrly5guBgK+9jJBFBn9cNplSZJ/oGuAVAnnrTeOE7uQrw5+0GiIiIpFKqg8zEiRMRFxeH69ev48CBA+jduzfkcjkGDBjg6NIAWPbIJGflO2Mp6ZRxg4BIQM7/2RIREUmlVA8t/fXXXxgwYADu378Pf39/PPvsszh06BD8/f0dXRoAQJbXIyOolOZryAS6Bz4MMsENHVUaERFRuVCqg8zKlSsdXcITyfJ6ZGQq1cPbE7gHATf2GTdgkCEiIpJUqR5aKu0e9sg8nCMT7Bacr0emkYMqIyIiKh8YZErAFGRkKvXDq/rKVEDWPeONIAMjHVkeERFRmccgUwLyvCAjVz/skQnK/Me40r82oHR1VGlERETlAoNMCZh6ZKCS437OfQBA0D9/GZcFN3BQVUREROUHg0wJyPXGyb4ahfFKwmq5Gj63LxpXcqIvERGR5BhkSkCR1yOTJTPeDyLYPRhC8mnjSgYZIiKn165dO4wfP17SYyxduhQ+Pj6SHsPaYwmCgLVr10pejy0wyJSA3BxkMgEAgS6+QNrfxpVB9R/3MiIiApCcnIxx48YhIiICLi4uCAwMROvWrbFw4cIi3zDQVvbs2QNBEJCSkmKxfPXq1fjkk09KvP+ffvoJcrkco0ePLvG+SqJ///74888/zc+nTZuGRo0aOa4gGyjV15Ep7RR5Q0sZQgYAIMjUnBUjAHXhd+cmIiLg6tWraN26NXx8fDBr1izUr18farUaZ86cwf/+9z9UqlQJL7zwgqPLhK+vr03289133+G9997D4sWLMXfuXIfc4FOr1cLV1RWurmXrRBT2yJSA3GDskUkTUwEAQbl5d93msBIROZAoisjSZtn9SxTFItc4atQoKBQKHDt2DP369UOdOnVQrVo1vPjii9i4cSN69uxp3jYlJQWvvvoq/P394eXlhQ4dOuDUqVPm9aZeheXLl6Nq1arw9vbGgAEDkJ6ebt7GYDAgJiYG4eHhcHV1RcOGDfHrr78CAK5fv4727dsDACpUqABBEDB06FAABYeWNBoN3n//fYSGhkKtViMiIgLffffdE9/rtWvXcODAAUyaNAk1a9bE6tWrn9o+n376KQICAuDp6YlXX30VkyZNsug5MRgMmDFjBipXrgy1Wo1GjRphy5Yt5vXXr1+HIAj4+eef0bZtW7i4uODHH3+0GFpaunQppk+fjlOnTkEQBAiCgKVLl5r3ce/ePfTu3Rtubm6oUaMG1q9fb15n6sHaunUr2rRpA3d3d3To0AF37tzB5s2bUadOHXh5eWHgwIGS966xR6YElHlDS/+IaQDynXodxDOWiMhxsnXZaLGihd2Pe3jgYbgp3Z663f3797Ft2zbMmjUL7u7uhW4jCIL58UsvvQRXV1ds3rwZ3t7eWLx4MTp27Ig///zT3GNy5coVrF27Fhs2bMA///yDfv364csvv8Tnn38OAIiJicEPP/yARYsWoUaNGti7dy9eeeUV8+1vfvvtN/Tt2xcJCQnw8vJ6bK/F4MGDcfDgQcyfPx8NGzbEtWvXcO/evSe+39jYWPTo0QPe3t545ZVX8N1332HgwIGP3f7HH3/EzJkz8fXXX6N169ZYuXIl5s6di/DwcPM28+bNw9y5c7F48WI0btwYS5YswQsvvIBz586hRo0a5u0mTZqEuXPnonHjxnBxccHWrVvN6/r374+zZ89iy5Yt2LFjBwDA29vbvH769On47LPP8Pnnn2PBggUYNGgQbty4YdFLNWPGDHz22Wfw9/fHyy+/jH79+kGtVmPFihXIyMhA7969sWDBArz//vtPbKOSYJApAUVej8wDgzHABJtPvWaPDBHR41y+fBmiKKJWrVoWy/38/JCTkwMAGD16NGbPno19+/bhyJEjuHPnDtRqNQBgzpw5WLt2LX799Ve8/vrrAIw9FEuXLoWnp3FY/5VXXsGePXsAGHtRZs2ahR07diAqKgoAUK1aNezbtw+LFy9G27ZtzR/OAQEBj50M++eff+KXX37B9u3bER0dbd7Pk5jqWrBgAQDg5ZdfxjvvvINr165ZBJP8FixYgBEjRmDYsGEAgI8++gjbtm1DRkaGeZs5c+bg/fffx8svvwwAmD17Nnbv3o0vv/wSX331lXm78ePHo0+fPoUex9XVFR4eHlAoFAgKCiqwfujQoeabNM+aNQvz58/HkSNH0LVrV/M2M2bMwDPPPAMvLy+MGDECkydPxpUrV8zt8n//93/YvXs3g0xpZTpr6a4+7xoyKQwyROR4rgpXHB542CHHLYkjR47AYDBg0KBB0GiMQ/WnTp1CRkYGKlasaLFtdnY2rly5Yn5etWpVc4gBgODgYNy9exeAMThlZWWhU6dOFvvIzc1F48aNi1xffHw85HI52rZtW+TXbN++HZmZmejevTsAY1jr1KkTlixZ8thJxAkJCRg1apTFsubNm2PXrl0AgLS0NNy6dQutW7e22KZ169YWQ24A0KxZsyLX+qgGDR6OLri7u8PLywt37tx57DaBgYFwc3OzCHeBgYE4cuRIsWsoCgaZYhJFEUqDcbJvGjIBCAjS6QHvKoCbbSaHEREVhyAIRRricZSIiAgIgoCEhASL5aYPwPzDOhkZGQgODjb3ruSXv+dEqVRarBMEAQaDwbwPANi4cSMqVapksZ2pl6coijNJ9rvvvsODBw8sXmswGHD69GlMnz4dMpm0U1UfN3RXFE9q08K2EQShSK+xNU72LSZRp4MMxoltWjngKVPBXRR5RV8ioqeoWLEiOnXqhP/+97/IzMx84rZNmjRBcnIyFAoFIiIiLL78/PyKdLzIyEio1WokJiYW2EdoaCgAQKVSAQD0eWejFqZ+/fowGAyIi4sr0nHv37+PdevWYeXKlYiPjzd/nTx5Ev/88w+2bdtW6Otq1aqFo0ePWizL/9zLywshISHYv3+/xTb79+9HZKR19/hTqVRPfM/OgD0yxZSbk2t+rJMDoZAbn/CO10RET2WayNqsWTNMmzYNDRo0gEwmw9GjR3Hx4kU0bdoUABAdHY2oqCj06tULn332GWrWrIlbt25h48aN6N27d5GGTjw9PTFx4kS8/fbbMBgMePbZZ5Gamor9+/fDy8sLQ4YMQVhYGARBwIYNG9C9e3fz/JH8qlatiiFDhmD48OHmyb43btzAnTt30K9fvwLHXb58OSpWrIh+/fpZTF4GgO7du+O7776zmG9iMnbsWLz22mto1qwZWrVqhZ9//hmnT5+2GLJ599138fHHH6N69epo1KgRYmNjER8fjx9//LFI7Z//PV27dg3x8fGoXLkyPD09reqlKg0YZIpJk5VjfqxV5D/1mj0yRERPU716dZw8eRKzZs3C5MmT8ddff0GtViMyMhITJ040zxERBAGbNm3CBx98gGHDhuHu3bsICgpCmzZtEBgYWOTjffLJJ/D390dMTAyuXr0KHx8fNGnSBFOmTAEAVKpUCdOnT8ekSZMwbNgwDB482OJUZJOFCxdiypQpGDVqFO7fv48qVaqY9/GoJUuWoHfv3gVCDAD07dsX//rXvwo942nQoEG4evUqJk6ciJycHPTr1w9Dhw61mGvy1ltvITU1Fe+88w7u3LmDyMhIrF+/3uKMpaLo27cvVq9ejfbt2yMlJQWxsbHmU8+dhSBac+K/E0pLS4O3tzdSU1Ph5eVls/3eunwDqc93hV4ABkxS4KX0DHx07wHwTgLgWXD2NxWfVqvFpk2b0L179wLjr2Q7bGf7sHU75+TkmM+AccRF1korg8GAtLQ0eHl5ST4PxR46deqEoKAgLF++3NGlWChpOz/p97eon9/skSmm3Gxjj4xObvzBBWl1gEcgQwwREZVIVlYWFi1ahC5dukAul+Onn37Cjh07sH37dkeXVioxyBSTNsc4lKTNmxoTpNfztGsiIiox03DazJkzkZOTg1q1auG3334zX7uGLDHIFJM2WwMBxom+ABCk0zHIEBFRibm6upqvtEtP5/wDhw5i7pFRGKcYBenYI0NERGRvDDLFpDMPLRmDTKCePTJERET2xiBTTNq8y2frFICvXg+12gfwDnVsUUREROUMg0wx6fN6ZHTyfPNjCrlWABEREUmHQaaYdBrjlX21cs6PISIichQGmWLSa0xzZAQGGSIiIgdhkCkmfV6PjE4OBOt0vMcSERGRAzDIFJMhN29oSQEECUrAt9pTXkFERPkNHToUgiBAEASoVCpERERgxowZ0Ol0AIBvvvkGDRs2hIeHB3x8fNC4cWPExMRY7OPBgwcYP348wsLCoFKpEBISguHDhyMxMbHIdZw8eRL9+/dHcHAw1Go1wsLC8Pzzz+P333+H6S4+169fN9cqCAI8PT1Rt25djB49GpcuXbJdo5DVeEG8YtLnPuyRCfKuCpSBe3kQEdlb165dERsbC41Gg02bNmH06NFQKpUIDAzE+PHjMX/+fLRt2xYajQanT5/G2bNnza998OABWrZsCZVKhUWLFqFu3bq4fv06PvzwQ7Ro0QJbt25FgwZPvpHvunXr0K9fP0RHR2PZsmWIiIiARqPBgQMH8OGHH+K5556Dj4+PefsdO3agbt26yMrKwpkzZzBv3jw0bNgQv//+Ozp27ChVM9ETMMgUk0Hz8BYFQQH1HVwNEdFDoihCzM62+3EFV9dC7/T8JGq1GkFBxnvUjRw5EmvWrMH69esRGBiIfv36YcSIEeZt69ata/HaDz74ALdu3cLly5fN+6hSpQq2bt2KGjVqYOLEidi2bdtjj52ZmYkRI0agR48eWL16tcW6OnXqYMSIEXj0vsoVK1Y0H6tatWro2bMnOnbsiBEjRuDKlSuQy+VWvX8qOQaZYsrNSQdg7JHxq9TcwdUQET0kZmcjoUlTux+31onjENzcSrQPV1dX3L9/H0FBQYiLi8ONGzcQFhZWYDuDwYCVK1di0KBB5mCRfx8jR47E1KlT8eDBA/j5+RV6rG3btuH+/ft47733HlvP04KZTCbDuHHj0Lt3bxw/fhzNm/PzwN44HlJMGk0KAEApE6EIaezYYoiInJwoitixYwe2bt2KDh064OOPP4aPjw+qVq2KWrVqYejQofjll19gMBgAAHfv3kVKSgrq1KlT6P5q164NURRx+fLlxx7zzz//BADUqlXLvOzo0aPw8PAwf23YsOGptdeuXRuAcR4N2R97ZIpJlnMHAKCSiYBfradsTURkP4KrK2qdOO6Q41prw4YN8PDwgFarhcFgwMCBAzFt2jS4u7vj4MGDOHv2LPbu3YsDBw5gyJAh+Pbbb7Flyxbz6x8d+nkcDw8P8+NXXnkFixYtKnS7Bg0aID4+HgBQo0YN88TjJzHVYO2wGtkGg0wxCTn3AAAKuQKQsxmJqPQQBKHEQzz20r59eyxcuNB8xpFCYfn3tF69eqhXrx5GjRqFN998E8899xzi4uLQtm1b+Pj44MKFC4Xu9+LFixAEAREREQBgDicA4OXlBcAYVAAgISEBLVu2BGCcs2N6TVGZaggPD7fqdWQbHFoqJkFrnCMjVzjHHwsiotLI3d0dERERqFKlSoEQ86jIyEgAxkm6MpkM/fr1w4oVK5CcnGyxXXZ2NhYuXIgOHTrA19cXABAREWH+CggIAAB07twZvr6+mD17drHrNxgMmD9/PsLDw9G4MacZOAK7EopJFAzIVQCC2sfRpRARlTkjR45ESEgIOnTogMqVKyMpKQmffvop/P39ERUVBQCYNWsWdu7ciU6dOuGzzz5DvXr1cO3aNXz44YfQarWYM2fOE4/h4eGBb7/9Fv3790ePHj3w1ltvoUaNGsjIyDAPXz16FtL9+/eRnJyMrKwsnD17Fl9++SWOHDmCjRs38owlB2GQKabeyw9h4+9r8UJnXjeAiMjWoqOjsWTJEixcuBD379+Hn58foqKisHPnTlSsWBGA8VToQ4cOYcaMGXjjjTeQnJwMX19fdOvWDd9//73F9V8ep3fv3jhw4ABmz56NwYMH48GDB/D29kazZs2wcuVKPP/88wXqAgA3NzeEhYWhffv2+N///mf1cBTZDoNMCQhyFZRqT0eXQUTklJYuXfrYdX379kXfvn2fug8/Pz/Mnz8f8+fPt1huMBiQlpZWpDqaNWuGVatWPXGbqlWrFnliMdkX58gQERGR02KQISIiIqfFIENEREROi0GGiIiInBaDDBFRGcCJqOSMbPF7yyBDROTElEolACArK8vBlRBZz/R7a/o9Lg6efk1E5MTkcjl8fHxw547x/m9ubm685w+Mp1/n5uYiJycHMhn/zy6V4razKIrIysrCnTt34OPjU6KLCTLIEBE5uaCgIAAwhxkyflBmZ2fD1dWVwU5CJW1nHx8f8+9vcTHIEBE5OUEQEBwcjICAAGi1WkeXUypotVrs3bsXbdq0KdGwBT1ZSdpZqVTa5LYODDJERGWEXC7n/X7yyOVy6HQ6uLi4MMhIqDS0MwcOiYiIyGkxyBAREZHTYpAhIiIip1Xm58iYLrZT1LugFpVWq0VWVhbS0tI4/ioxtrV9sJ3tg+1sH2xn+5CynU2f20+7aF6ZDzLp6ekAgNDQUAdXQkRERNZKT0+Ht7f3Y9cLYhm/rrXBYMCtW7fg6elp02sJpKWlITQ0FDdv3oSXl5fN9ksFsa3tg+1sH2xn+2A724eU7SyKItLT0xESEvLEi+2V+R4ZmUyGypUrS7Z/Ly8v/iOxE7a1fbCd7YPtbB9sZ/uQqp2f1BNjwsm+RERE5LQYZIiIiMhpMcgUk1qtxscffwy1Wu3oUso8trV9sJ3tg+1sH2xn+ygN7VzmJ/sSERFR2cUeGSIiInJaDDJERETktBhkiIiIyGkxyBAREZHTYpAppq+++gpVq1aFi4sLWrRogSNHjji6JKeyd+9e9OzZEyEhIRAEAWvXrrVYL4oiPvroIwQHB8PV1RXR0dG4dOmSxTYPHjzAoEGD4OXlBR8fH4wYMQIZGRl2fBelX0xMDJ555hl4enoiICAAvXr1QkJCgsU2OTk5GD16NCpWrAgPDw/07dsXt2/fttgmMTERPXr0gJubGwICAvDuu+9Cp9PZ862UagsXLkSDBg3MFwWLiorC5s2bzevZxtL497//DUEQMH78ePMytnXJTZs2DYIgWHzVrl3bvL7UtbFIVlu5cqWoUqnEJUuWiOfOnRNfe+010cfHR7x9+7ajS3MamzZtEj/44ANx9erVIgBxzZo1Fuv//e9/i97e3uLatWvFU6dOiS+88IIYHh4uZmdnm7fp2rWr2LBhQ/HQoUPiH3/8IUZERIgDBgyw8zsp3bp06SLGxsaKZ8+eFePj48Xu3buLVapUETMyMszbvPnmm2JoaKi4c+dO8dixY2LLli3FVq1amdfrdDqxXr16YnR0tHjy5Elx06ZNop+fnzh58mRHvKVSaf369eLGjRvFP//8U0xISBCnTJkiKpVK8ezZs6Ioso2lcOTIEbFq1apigwYNxHHjxpmXs61L7uOPPxbr1q0rJiUlmb/u3r1rXl/a2phBphiaN28ujh492vxcr9eLISEhYkxMjAOrcl6PBhmDwSAGBQWJn3/+uXlZSkqKqFarxZ9++kkURVE8f/68CEA8evSoeZvNmzeLgiCIf//9t91qdzZ37twRAYhxcXGiKBrbValUiqtWrTJvc+HCBRGAePDgQVEUjaFTJpOJycnJ5m0WLlwoenl5iRqNxr5vwIlUqFBB/Pbbb9nGEkhPTxdr1Kghbt++XWzbtq05yLCtbePjjz8WGzZsWOi60tjGHFqyUm5uLo4fP47o6GjzMplMhujoaBw8eNCBlZUd165dQ3JyskUbe3t7o0WLFuY2PnjwIHx8fNCsWTPzNtHR0ZDJZDh8+LDda3YWqampAABfX18AwPHjx6HVai3aunbt2qhSpYpFW9evXx+BgYHmbbp06YK0tDScO3fOjtU7B71ej5UrVyIzMxNRUVFsYwmMHj0aPXr0sGhTgL/PtnTp0iWEhISgWrVqGDRoEBITEwGUzjYu8zeNtLV79+5Br9db/IAAIDAwEBcvXnRQVWVLcnIyABTaxqZ1ycnJCAgIsFivUCjg6+tr3oYsGQwGjB8/Hq1bt0a9evUAGNtRpVLBx8fHYttH27qwn4VpHRmdOXMGUVFRyMnJgYeHB9asWYPIyEjEx8ezjW1o5cqVOHHiBI4ePVpgHX+fbaNFixZYunQpatWqhaSkJEyfPh3PPfcczp49WyrbmEGGqJwYPXo0zp49i3379jm6lDKpVq1aiI+PR2pqKn799VcMGTIEcXFxji6rTLl58ybGjRuH7du3w8XFxdHllFndunUzP27QoAFatGiBsLAw/PLLL3B1dXVgZYXj0JKV/Pz8IJfLC8zQvn37NoKCghxUVdliascntXFQUBDu3LljsV6n0+HBgwf8ORRizJgx2LBhA3bv3o3KlSublwcFBSE3NxcpKSkW2z/a1oX9LEzryEilUiEiIgJNmzZFTEwMGjZsiHnz5rGNbej48eO4c+cOmjRpAoVCAYVCgbi4OMyfPx8KhQKBgYFsawn4+PigZs2auHz5cqn8fWaQsZJKpULTpk2xc+dO8zKDwYCdO3ciKirKgZWVHeHh4QgKCrJo47S0NBw+fNjcxlFRUUhJScHx48fN2+zatQsGgwEtWrSwe82llSiKGDNmDNasWYNdu3YhPDzcYn3Tpk2hVCot2johIQGJiYkWbX3mzBmL4Lh9+3Z4eXkhMjLSPm/ECRkMBmg0GraxDXXs2BFnzpxBfHy8+atZs2YYNGiQ+THb2vYyMjJw5coVBAcHl87fZ5tPHy4HVq5cKarVanHp0qXi+fPnxddff1308fGxmKFNT5aeni6ePHlSPHnypAhA/OKLL8STJ0+KN27cEEXRePq1j4+PuG7dOvH06dPiiy++WOjp140bNxYPHz4s7tu3T6xRowZPv37EyJEjRW9vb3HPnj0Wp1JmZWWZt3nzzTfFKlWqiLt27RKPHTsmRkVFiVFRUeb1plMpO3fuLMbHx4tbtmwR/f39ebpqPpMmTRLj4uLEa9euiadPnxYnTZokCoIgbtu2TRRFtrGU8p+1JIpsa1t45513xD179ojXrl0T9+/fL0ZHR4t+fn7inTt3RFEsfW3MIFNMCxYsEKtUqSKqVCqxefPm4qFDhxxdklPZvXu3CKDA15AhQ0RRNJ6CPXXqVDEwMFBUq9Vix44dxYSEBIt93L9/XxwwYIDo4eEhenl5icOGDRPT09Md8G5Kr8LaGIAYGxtr3iY7O1scNWqUWKFCBdHNzU3s3bu3mJSUZLGf69evi926dRNdXV1FPz8/8Z133hG1Wq2d303pNXz4cDEsLExUqVSiv7+/2LFjR3OIEUW2sZQeDTJs65Lr37+/GBwcLKpUKrFSpUpi//79xcuXL5vXl7Y2FkRRFG3fz0NEREQkPc6RISIiIqfFIENEREROi0GGiIiInBaDDBERETktBhkiIiJyWgwyRERE5LQYZIiIiMhpMcgQERGR02KQISK7qlq1Kr788ktHlyGZpUuXwsfHx9FlEJUbDDJEZdTQoUPRq1cv8/N27dph/Pjxdjv+4z7Qjx49itdff91udRBR2cYgQ0RWyc3NLdHr/f394ebmZqNqyg+tVuvoEohKJQYZonJg6NChiIuLw7x58yAIAgRBwPXr1wEAZ8+eRbdu3eDh4YHAwED861//wr1798yvbdeuHcaMGYPx48fDz88PXbp0AQB88cUXqF+/Ptzd3REaGopRo0YhIyMDALBnzx4MGzYMqamp5uNNmzYNQMGhpcTERLz44ovw8PCAl5cX+vXrh9u3b5vXT5s2DY0aNcLy5ctRtWpVeHt74+WXX0Z6evpj36+pN2jr1q2oU6cOPDw80LVrVyQlJVm8r0d7qHr16oWhQ4ean1etWhWffvopBg8eDA8PD4SFhWH9+vW4e/euueYGDRrg2LFjBWpYu3YtatSoARcXF3Tp0gU3b960WL9u3To0adIELi4uqFatGqZPnw6dTmdeLwgCFi5ciBdeeAHu7u6YOXPmY98vUXnGIENUDsybNw9RUVF47bXXkJSUhKSkJISGhiIlJQUdOnRA48aNcezYMWzZsgW3b99Gv379LF6/bNkyqFQq7N+/H4sWLQIAyGQyzJ8/H+fOncOyZcuwa9cuvPfeewCAVq1a4csvv4SXl5f5eBMnTixQl8FgwIsvvogHDx4gLi4O27dvx9WrV9G/f3+L7a5cuYK1a9diw4YN2LBhA+Li4vDvf//7ie85KysLc+bMwfLly7F3714kJiYWWsPT/Oc//0Hr1q1x8uRJ9OjRA//6178wePBgvPLKKzhx4gSqV6+OwYMHI//9d7OysjBz5kx8//332L9/P1JSUvDyyy+b1//xxx8YPHgwxo0bh/Pnz2Px4sVYunRpgbAybdo09O7dG2fOnMHw4cOtrp2oXJDkntpE5HBDhgwRX3zxRfPztm3biuPGjbPY5pNPPhE7d+5ssezmzZsiADEhIcH8usaNGz/1eKtWrRIrVqxofh4bGyt6e3sX2C4sLEz8z3/+I4qiKG7btk2Uy+ViYmKief25c+dEAOKRI0dEURTFjz/+WHRzcxPT0tLM27z77rtiixYtHltLbGysCEC8fPmyedlXX30lBgYGmp8X1h4vvviiOGTIEItaX3nlFfPzpKQkEYA4depU87KDBw+KAMSkpCSLYx86dMi8zYULF0QA4uHDh0VRFMWOHTuKs2bNsjj28uXLxeDgYPNzAOL48eMf+x6JyEjhuAhFRI526tQp7N69Gx4eHgXWXblyBTVr1gQANG3atMD6HTt2ICYmBhcvXkRaWhp0Oh1ycnKQlZVV5DkwFy5cQGhoKEJDQ83LIiMj4ePjgwsXLuCZZ54BYBzi8fT0NG8THByMO3fuPHHfbm5uqF69ulWvKUyDBg3MjwMDAwEA9evXL7Dszp07CAoKAgAoFApz7QBQu3Zt83tq3rw5Tp06hf3791v0wOj1+gLt16xZM6vrJSpvGGSIyrGMjAz07NkTs2fPLrAuODjY/Njd3d1i3fXr1/H8889j5MiRmDlzJnx9fbFv3z6MGDECubm5Np/Mq1QqLZ4LggCDwWD1a8R8wz8ymcziOVD4hNr8+xEE4bHLnlZPfhkZGZg+fTr69OlTYJ2Li4v58aPtTkQFMcgQlRMqlQp6vd5iWZMmTfDbb7+hatWqUCiK/ufg+PHjMBgMmDt3LmQy41S7X3755anHe1SdOnVw8+ZN3Lx509wrc/78eaSkpCAyMrLI9RSHv7+/xeRfvV6Ps2fPon379iXet06nw7Fjx9C8eXMAQEJCAlJSUlCnTh0AxnZPSEhAREREiY9FVN5xsi9ROVG1alUcPnwY169fx71792AwGDB69Gg8ePAAAwYMwNGjR3HlyhVs3boVw4YNe2IIiYiIgFarxYIFC3D16lUsX77cPAk4//EyMjKwc+dO3Lt3D1lZWQX2Ex0djfr162PQoEE4ceIEjhw5gsGDB6Nt27aSD6t06NABGzduxMaNG3Hx4kWMHDkSKSkpNtm3UqnE2LFjcfjwYRw/fhxDhw5Fy5YtzcHmo48+wvfff4/p06fj3LlzuHDhAlauXIkPP/zQJscnKk8YZIjKiYkTJ0IulyMyMhL+/v5ITExESEgI9u/fD71ej86dO6N+/foYP348fHx8zD0thWnYsCG++OILzJ49G/Xq1cOPP/6ImJgYi21atWqFN998E/3794e/vz8+++yzAvsRBAHr1q1DhQoV0KZNG0RHR6NatWr4+eefbf7+HzV8+HAMGTLEHJyqVatmk94YwDg/5/3338fAgQPRunVreHh4WLynLl26YMOGDdi2bRueeeYZtGzZEv/5z38QFhZmk+MTlSeC+OggMREREZGTYI8MEREROS0GGSIiInJaDDJERETktBhkiIiIyGkxyBAREZHTYpAhIiIip8UgQ0RERE6LQYaIiIicFoMMEREROS0GGSIiInJaDDJERETktP4fxwH1BovkiYMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, num_cycles+10, 10)\n",
    "\n",
    "# Plotting the data\n",
    "plt.plot(iterations, GD_results, label='Gradient Descent')\n",
    "plt.plot(iterations, PSO_results, label='PSO')\n",
    "plt.plot(iterations, GA_results[0:len(iterations)], label='Genetic Algorithm')\n",
    "plt.plot(iterations, PSO_GD_results, label='PSO-GD')\n",
    "\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Secrecy rate (bps/Hz)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Secrecy Rate GD: [24.6548294]\n",
      "Best Secrecy Rate PSO: [11.27448635]\n",
      "Best Secrecy Rate GA: [17.00556469]\n",
      "Best Secrecy Rate PSO-GD: [25.62364522]\n"
     ]
    }
   ],
   "source": [
    "# Best results of each methods\n",
    "print(\"Best Secrecy Rate GD:\", max(GD_results))\n",
    "print(\"Best Secrecy Rate PSO:\", max(PSO_results))\n",
    "print(\"Best Secrecy Rate GA:\", max(GA_results))\n",
    "print(\"Best Secrecy Rate PSO-GD:\", max(PSO_GD_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
